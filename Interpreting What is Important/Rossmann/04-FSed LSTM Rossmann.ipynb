{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c3889da",
   "metadata": {},
   "outputs": [],
   "source": [
    "#%pip install jupyter_tensorboard"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6fc27058",
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns  #Visualization\n",
    "from pylab import rcParams\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib import rc\n",
    "import matplotlib\n",
    "\n",
    "\n",
    "import pandas as pd   #preprocessing\n",
    "import numpy as np\n",
    "import math\n",
    "from tqdm.notebook import tqdm\n",
    "import pytorch_lightning as pl\n",
    "from pytorch_lightning.callbacks import ModelCheckpoint, EarlyStopping\n",
    "from pytorch_lightning.loggers import TensorBoardLogger\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "\n",
    "import torch          #modelling\n",
    "import torch.autograd as autograd\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from collections import defaultdict\n",
    "\n",
    "\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error\n",
    "\n",
    "#import operator\n",
    "import time\n",
    "import os\n",
    "\n",
    "from IPython.utils import io\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\", \".*does not have many workers.*\")\n",
    "warnings.filterwarnings(\"ignore\", category=DeprecationWarning) \n",
    "warnings.filterwarnings(\"ignore\", category=UserWarning) \n",
    "\n",
    "\n",
    "import tensorboard\n",
    "%reload_ext tensorboard\n",
    "\n",
    "\n",
    "def mean_absolute_scaled_error(y_true, y_pred, y_train):\n",
    "    e_t = y_true - y_pred\n",
    "    scale = mean_absolute_error(y_train[1:], y_train[:-1])\n",
    "    return np.mean(np.abs(e_t / scale))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0420849e",
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "%config InlineBackend.figure_format='retina'\n",
    "\n",
    "sns.set(style='whitegrid',palette='muted',font_scale=1.2)\n",
    "\n",
    "rcParams['figure.figsize']=12,8\n",
    "\n",
    "tqdm.pandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9566829a",
   "metadata": {},
   "outputs": [],
   "source": [
    "N_EPOCHS = 220\n",
    "\n",
    "arr = np.arange(14, N_EPOCHS, 5)\n",
    "# Convert the elements of the array to strings and create a list\n",
    "checkpoints = [str(x) for x in arr]\n",
    "\n",
    "BATCH_SIZE = 7\n",
    "SEQUENCE_LENGTH = 15\n",
    "N_HIDDEN = 32\n",
    "N_LAYERS = 1\n",
    "PATIENCE = 500\n",
    "LEARNING = 0.005\n",
    "\n",
    "TGT = 'Sales'\n",
    "\n",
    "all_features = False\n",
    "univariate = False\n",
    "\n",
    "corr_train = False\n",
    "\n",
    "Boruta_GB = False\n",
    "Boruta_RF = False\n",
    "\n",
    "BorutaSHAP_GB = False\n",
    "BorutaSHAP_RF = False\n",
    "\n",
    "IMV_Tensor = False \n",
    "IMV_Full = False \n",
    "\n",
    "LIME_train = False\n",
    "\n",
    "SHAP_insta = False\n",
    "SHAP_avrag = False "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7e6e562",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Dataset(Dataset):\n",
    "    def __init__(self,sequences):\n",
    "        self.sequences = sequences\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.sequences)\n",
    "\n",
    "    def __getitem__(self,idx):\n",
    "        sequence, label = self.sequences[idx]\n",
    "\n",
    "        return dict(sequence = torch.Tensor(sequence.to_numpy()),\n",
    "            label = torch.tensor(label).float())\n",
    "\n",
    "class DataModule(pl.LightningDataModule):\n",
    "    def __init__(self, train_seqeunces,test_sequences, batch_size=8):\n",
    "        super().__init__()\n",
    "        self.train_sequences = train_sequences\n",
    "        self.test_sequences = test_sequences\n",
    "        self.batch_size = batch_size\n",
    "\n",
    "    def setup(self,stage=None):\n",
    "        self.train_dataset = Dataset(self.train_sequences)\n",
    "        self.test_dataset = Dataset(self.test_sequences)\n",
    "\n",
    "    def train_dataloader(self):\n",
    "        return DataLoader(\n",
    "            self.train_dataset,\n",
    "            batch_size = self.batch_size,\n",
    "            shuffle = False,\n",
    "            num_workers = 0)\n",
    "\n",
    "    def val_dataloader(self):\n",
    "        return DataLoader(\n",
    "            self.test_dataset,\n",
    "            batch_size = 1,\n",
    "            shuffle = False,\n",
    "            num_workers = 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71f38fe9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def features_dataframe(df,features):\n",
    "    rows = []\n",
    "\n",
    "    for _, row in df.iterrows():\n",
    "        row_data = dict(\n",
    "            Sales = row[f'{TGT}'],\n",
    "        )\n",
    "        for column in features:\n",
    "            row_data[column] = row[column]\n",
    "            \n",
    "        rows.append(row_data)\n",
    "    \n",
    "    features_df = pd.DataFrame(rows)\n",
    "\n",
    "    return features_df\n",
    "\n",
    "#spliits the data in test and train\n",
    "def train_test_spliter(ratio,features_df ):\n",
    "    train_size = int(len(features_df)-ratio)\n",
    "    train_df, test_df = features_df[:train_size], features_df[train_size + 1:]\n",
    "\n",
    "    return train_df, test_df, train_size\n",
    "\n",
    "def data_scaler(train_df,test_df):\n",
    "    scaler = MinMaxScaler(feature_range=(-1,1))\n",
    "    scaler = scaler.fit(train_df)\n",
    "    \n",
    "    train_df = pd.DataFrame(\n",
    "        scaler.transform(train_df),\n",
    "        index = train_df.index,\n",
    "        columns = train_df.columns)\n",
    "\n",
    "    test_df = pd.DataFrame(\n",
    "        scaler.transform(test_df),\n",
    "        index = test_df.index,\n",
    "        columns = test_df.columns)\n",
    "    \n",
    "    return train_df, test_df, scaler\n",
    "\n",
    "\n",
    "def create_sequences (input_data:pd.DataFrame, target_column, sequence_length):\n",
    "    sequences = []\n",
    "    data_size = len(input_data)\n",
    "\n",
    "    for i in (range(data_size - sequence_length)):\n",
    "        sequence = input_data[i:i+sequence_length]\n",
    "        label_position = i + sequence_length\n",
    "        label = input_data.iloc[label_position][target_column]\n",
    "        sequences.append((sequence,label))\n",
    "    \n",
    "    return sequences\n",
    "\n",
    "def descale(descaler, values):\n",
    "    values_2d=np.array(values)[:,np.newaxis]\n",
    "    \n",
    "    return descaler.inverse_transform(values_2d).flatten()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "466e2586",
   "metadata": {},
   "outputs": [],
   "source": [
    "def def_model():\n",
    "    class PredictionModel(nn.Module):\n",
    "        def __init__(self, n_features, n_hidden = N_HIDDEN, n_layers = N_LAYERS):\n",
    "            super().__init__()\n",
    "            self.n_hidden = n_hidden\n",
    "            self.lstm = nn.LSTM(\n",
    "                input_size = n_features,\n",
    "                hidden_size = n_hidden,\n",
    "                batch_first = True,\n",
    "                num_layers = n_layers,\n",
    "                dropout = 0.2)\n",
    "            self.regressor = nn.Linear(n_hidden,1)\n",
    "\n",
    "        def forward(self,x):\n",
    "            self.lstm.flatten_parameters()\n",
    "            _, (hidden, _) = self.lstm(x)\n",
    "            out = hidden[-1]\n",
    "            return self.regressor(out)\n",
    "        \n",
    "\n",
    "    class Predictor(pl.LightningModule):\n",
    "        def __init__(self, n_features: int):\n",
    "            super().__init__()\n",
    "            self.model=PredictionModel(n_features)\n",
    "            self.criterion = nn.MSELoss()\n",
    "\n",
    "        def forward(self, x, labels= None):\n",
    "            output = self.model(x)\n",
    "            loss = 0\n",
    "            if labels is not None:\n",
    "                loss = self.criterion(output, labels.unsqueeze(dim=1))\n",
    "            return loss, output\n",
    "        \n",
    "        def training_step(self, batch, batch_index):\n",
    "            sequences = batch['sequence']\n",
    "            labels = batch['label']\n",
    "            loss, outputs = self(sequences, labels)\n",
    "            self.log('train_loss', loss, prog_bar = False, logger=True,on_step=False, on_epoch=True)\n",
    "            return loss\n",
    "\n",
    "        def validation_step(self, batch, batch_index):\n",
    "            sequences = batch['sequence']\n",
    "            labels = batch['label']\n",
    "            loss, outputs = self(sequences, labels)\n",
    "            self.log('val_loss', loss, prog_bar = False, logger=True, on_step=False, on_epoch=True)\n",
    "            return loss\n",
    "\n",
    "        def test_step(self, batch, batch_index):\n",
    "            sequences = batch['sequence']\n",
    "            labels = batch['label']\n",
    "            loss, outputs = self(sequences, labels)\n",
    "            self.log('val_loss', loss, prog_bar = False, logger=True, on_step=False, on_epoch=True)\n",
    "            return loss\n",
    "\n",
    "        def configure_optimizers(self):\n",
    "            return optim.AdamW(self.parameters(), lr = LEARNING)\n",
    "    return PredictionModel, Predictor\n",
    "        \n",
    "        \n",
    "def pep_proc_summarizer(features,df):\n",
    "    features_df = features_dataframe(df,features) \n",
    "    #returns dataframe with the features to be analised\n",
    "    \n",
    "    #split into test and train and minmaxscaler\n",
    "    train_df, test_df, train_size =  train_test_spliter(105,features_df)\n",
    "    train_df, test_df, scaler = data_scaler(train_df,test_df)\n",
    "    \n",
    "    #make sequences with the data\n",
    "    train_sequences = create_sequences(train_df,TGT,SEQUENCE_LENGTH)\n",
    "    test_sequences = create_sequences (test_df,TGT,SEQUENCE_LENGTH)\n",
    "    train_dataset = Dataset(train_sequences)\n",
    "    test_dataset = Dataset(test_sequences)\n",
    "    \n",
    "    return (train_df,train_sequences,test_sequences,train_size,train_dataset,test_dataset,scaler)\n",
    "\n",
    "def def_trainer(folder_path1,folder_path2,filename,logs,store):\n",
    "    checkpoint_callback = ModelCheckpoint(\n",
    "        dirpath = f'{folder_path1}/{folder_path2}/Checkpoints',\n",
    "        filename = filename+'_{epoch:02d}',\n",
    "        save_top_k = len(checkpoints)+2,\n",
    "        verbose = False,\n",
    "        monitor='epoch',\n",
    "        every_n_epochs = 5)\n",
    "    \n",
    "    logger = TensorBoardLogger(f'{folder_path1}/{logs}', name = f'{store}Sales')\n",
    "    early_stopping_callback = EarlyStopping(monitor= 'val_loss', patience = PATIENCE)\n",
    "    \n",
    "    trainer = pl.Trainer(\n",
    "        logger = logger,\n",
    "        callbacks=[early_stopping_callback, checkpoint_callback],\n",
    "        max_epochs = N_EPOCHS,\n",
    "        gpus = 0,)\n",
    "    return trainer\n",
    "    \n",
    "def recal_predict(folder_path1,folder_path2,filename,features,epoch,test_dataset):\n",
    "    trained_model = Predictor.load_from_checkpoint(\n",
    "            f'{folder_path1}/{folder_path2}/Checkpoints/{filename}_epoch={epoch}.ckpt',\n",
    "            n_features = len(features))\n",
    "\n",
    "    predictions = []\n",
    "    labels = []\n",
    "\n",
    "    for item in test_dataset:\n",
    "        sequence = item['sequence']\n",
    "        label = item['label']\n",
    "\n",
    "        if len(predictions) > SEQUENCE_LENGTH:\n",
    "            for j in range(SEQUENCE_LENGTH):\n",
    "                sequence[-SEQUENCE_LENGTH+j,0] = float(predictions[-SEQUENCE_LENGTH+j])\n",
    "        else: \n",
    "            for j in range(len(predictions)):\n",
    "                sequence[-len(predictions)+j,0] = float(predictions[-len(predictions)+j])\n",
    "\n",
    "        _,output = trained_model(sequence.unsqueeze(dim=0))\n",
    "        predictions.append(output.item())\n",
    "        labels.append(label.item())\n",
    "    \n",
    "    return (predictions)\n",
    "\n",
    "def descale_(predictions):\n",
    "    descaler = MinMaxScaler()\n",
    "    descaler.min_, descaler.scale_ = scaler.min_[0], scaler.scale_[0]\n",
    "    predictions_descaled = descale(descaler,predictions)\n",
    "    \n",
    "    return (predictions_descaled)\n",
    "    \n",
    "def calculate_errors(predictions_descaled,df,train_size):\n",
    "    test_data = df[train_size+1:]\n",
    "    test_sequences_data = test_data.iloc[SEQUENCE_LENGTH:]\n",
    "\n",
    "    dates = matplotlib.dates.date2num(Rossmann_df.iloc[-len(predictions_descaled):].Date)\n",
    "    full_dates = matplotlib.dates.date2num(Rossmann_df.Date.tolist())\n",
    "    \n",
    "    dic = {}\n",
    "    dic[f'store_truth'] = Rossmann_df[TGT]\n",
    "    dic[f'store_truth_dates'] = full_dates\n",
    "    truth_df = pd.DataFrame.from_dict(dic)\n",
    "\n",
    "    dic= {}\n",
    "    dic[f'store_pred'] = predictions_descaled\n",
    "    dic[f'store_pred_dates'] = dates\n",
    "    prediction_df = pd.DataFrame.from_dict(dic)\n",
    "    prediction_df['store_pred'] =prediction_df['store_pred'].shift(-1)\n",
    "\n",
    "    #plt.figure(figsize=(21, 7))\n",
    "    #plt.plot_date(truth_df.iloc[-3*len(prediction_df):,1],truth_df.iloc[-3*len(prediction_df):,0],'-', label='Truth')\n",
    "    #plt.plot_date(prediction_df.iloc[:,1],prediction_df.iloc[:,0],'-',label ='Prediction')\n",
    "    #plt.legend()\n",
    "    #plt.show();\n",
    "\n",
    "    predictions_descaled = np.where(predictions_descaled<0, 0, predictions_descaled)\n",
    "    mean_abs_corrf=(mean_absolute_scaled_error(truth_df.iloc[-len(prediction_df):,0],\n",
    "                                     predictions_descaled,\n",
    "                                     truth_df.iloc[:-len(predictions_descaled),0]))\n",
    "    root_mean_corrf=(mean_squared_error(truth_df.iloc[-len(prediction_df):,0],\n",
    "                                    predictions_descaled)**(1/2))\n",
    "    \n",
    "    return (mean_abs_corrf,root_mean_corrf)\n",
    "\n",
    "def mk_dir(folder_path1,folder_path2):\n",
    "    try:\n",
    "        os.mkdir(f'{folder_path1}')\n",
    "    except:\n",
    "        pass\n",
    "    try:\n",
    "        os.mkdir(f'{folder_path1}/{folder_path2}')\n",
    "    except:\n",
    "        pass\n",
    "    try:\n",
    "        os.mkdir(f'{folder_path1}/{folder_path2}/Forecast')\n",
    "    except:\n",
    "        pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b36ac17",
   "metadata": {},
   "outputs": [],
   "source": [
    "Rossmann = pd.read_csv('01-Data/Rossmann_treated.csv')\n",
    "Rosmann_stores = [23,64,103,133,135,216,224,256,266,311,355,405,455,487,558,649,672,714,738,742,773,785,\n",
    "             830,870,880,899,937,978,1011,1069]\n",
    "\n",
    "Rossmann.drop(columns = 'Customers', inplace = True)\n",
    "\n",
    "folder_path1='02-Results/LSTM'\n",
    "try:\n",
    "    os.mkdir(f'{folder_path1}')\n",
    "except:\n",
    "    pass\n",
    "\n",
    "for store in tqdm(Rosmann_stores):\n",
    "    try:\n",
    "        os.mkdir(f'{folder_path1}/Store{store}')\n",
    "    except:\n",
    "        pass"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6fe8c2cf",
   "metadata": {},
   "source": [
    "## LSTM Using All Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6c2ebc1",
   "metadata": {},
   "outputs": [],
   "source": [
    "if all_features == True:\n",
    "    for store in tqdm(Rosmann_stores):\n",
    "        torch.manual_seed(42);\n",
    "        np.random.seed(42);\n",
    "        pl.seed_everything(42);  \n",
    "\n",
    "        Rossmann_df = Rossmann[Rossmann['Store'] == store]\n",
    "        folder_path2=f'Store{store}/All_Features'\n",
    "        filename=f'Rossmann_All_LSTM'\n",
    "        logs='logs_AllF'\n",
    "\n",
    "        mk_dir(folder_path1,folder_path2)\n",
    "\n",
    "        df = Rossmann_df.drop(columns  = 'Date')\n",
    "        features = list(df.columns)\n",
    "\n",
    "        train_df,train_sequences,test_sequences,train_size,train_dataset,test_dataset,scaler = pep_proc_summarizer(features,df)\n",
    "\n",
    "        PredictionModel, Predictor = def_model()\n",
    "        data_module = DataModule(train_sequences, test_sequences, batch_size = BATCH_SIZE)\n",
    "        trainer = def_trainer(folder_path1,folder_path2,filename,logs,store)\n",
    "        model = Predictor(n_features = len(train_df.columns))\n",
    "\n",
    "        iteration_start = time.monotonic()\n",
    "\n",
    "        trainer.fit(model, data_module)\n",
    "\n",
    "\n",
    "        iteration_end = time.monotonic()\n",
    "        \n",
    "        Error_dic = {}\n",
    "        MASE=[]\n",
    "        RMSE=[]\n",
    "\n",
    "        for i in checkpoints:\n",
    "            predictions = recal_predict(folder_path1,folder_path2,filename,train_df.columns,i,test_dataset)\n",
    "            predictions_descaled = descale_(predictions)\n",
    "            csv_dataframe = pd.DataFrame(predictions_descaled, columns=[f'Rosmann{store}_epoch={i}'])\n",
    "            csv_dataframe.to_csv(f'{folder_path1}/{folder_path2}/Forecast/Rossmann{store}_epoch{i}.csv')\n",
    "            mase,rmse=calculate_errors(predictions_descaled,df,train_size)\n",
    "            MASE.append(mase)\n",
    "            RMSE.append(rmse)\n",
    "\n",
    "        Error_dic['MASE'] = MASE\n",
    "        Error_dic['RMSE'] = RMSE\n",
    "        Error_dic['TIME'] =iteration_end-iteration_start\n",
    "        Error_df = pd.DataFrame.from_dict(Error_dic)\n",
    "        Error_df.to_csv(f'{folder_path1}/{folder_path2}/Errors.csv')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "abfc40a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%capture\n",
    "%tensorboard --logdir=./02-Results/LSTM/logs_AllF --host localhost --port=3000"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c2c1df64",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## LSTM Univariate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39a8cd02",
   "metadata": {},
   "outputs": [],
   "source": [
    "if univariate == True:\n",
    "    for store in tqdm(Rosmann_stores):\n",
    "        torch.manual_seed(42);\n",
    "        np.random.seed(42);\n",
    "        pl.seed_everything(42);  \n",
    "\n",
    "        Rossmann_df = Rossmann[Rossmann['Store'] == store]\n",
    "        folder_path2=f'Store{store}/univariate'\n",
    "        filename=f'Rossmann_univariate_LSTM'\n",
    "        logs='logs_univariate'\n",
    "\n",
    "        mk_dir(folder_path1,folder_path2)\n",
    "\n",
    "        df = Rossmann_df.drop(columns  = 'Date')\n",
    "        features = [TGT]\n",
    "\n",
    "        train_df,train_sequences,test_sequences,train_size,train_dataset,test_dataset,scaler = pep_proc_summarizer(features,df)\n",
    "\n",
    "        PredictionModel, Predictor = def_model()\n",
    "        data_module = DataModule(train_sequences, test_sequences, batch_size = BATCH_SIZE)\n",
    "        trainer = def_trainer(folder_path1,folder_path2,filename,logs,store)\n",
    "        model = Predictor(n_features = len(train_df.columns))\n",
    "\n",
    "        iteration_start = time.monotonic()\n",
    "\n",
    "        trainer.fit(model, data_module)\n",
    "\n",
    "\n",
    "        iteration_end = time.monotonic()\n",
    "        \n",
    "        Error_dic = {}\n",
    "        MASE=[]\n",
    "        RMSE=[]\n",
    "\n",
    "        for i in checkpoints:\n",
    "            predictions = recal_predict(folder_path1,folder_path2,filename,train_df.columns,i,test_dataset)\n",
    "            predictions_descaled = descale_(predictions)\n",
    "            csv_dataframe = pd.DataFrame(predictions_descaled, columns=[f'Rosmann{store}_epoch={i}'])\n",
    "            csv_dataframe.to_csv(f'{folder_path1}/{folder_path2}/Forecast/Rossmann{store}_epoch{i}.csv')\n",
    "            mase,rmse=calculate_errors(predictions_descaled,df,train_size)\n",
    "            MASE.append(mase)\n",
    "            RMSE.append(rmse)\n",
    "\n",
    "        Error_dic['MASE'] = MASE\n",
    "        Error_dic['RMSE'] = RMSE\n",
    "        Error_dic['TIME'] =iteration_end-iteration_start\n",
    "        Error_df = pd.DataFrame.from_dict(Error_dic)\n",
    "        Error_df.to_csv(f'{folder_path1}/{folder_path2}/Errors.csv')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2d16746",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "%%capture\n",
    "%tensorboard --logdir=./02-Results/LSTM/logs_univariate --host localhost --port=3001"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d9be358b",
   "metadata": {},
   "source": [
    "## LSTM Correlation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae90a805",
   "metadata": {},
   "outputs": [],
   "source": [
    "corr_bsline = pd.read_csv(f'02-Results/FS/Store{store}/Correlation/correlation_df.csv')\n",
    "corr_bsline.drop(columns = 'all_features', inplace = True)\n",
    "\n",
    "if corr_train == True:\n",
    "    for corr_col in corr_bsline.columns[-1:]:\n",
    "        for store in tqdm(Rosmann_stores):        \n",
    "            \n",
    "            corr_feat = pd.read_csv(f'02-Results/FS/Store{store}/Correlation/correlation_df.csv')\n",
    "            corr_feat.drop(columns = 'all_features', inplace = True)\n",
    "            corr_dic = {}\n",
    "\n",
    "            torch.manual_seed(42);\n",
    "            np.random.seed(42);\n",
    "            pl.seed_everything(42);  \n",
    "\n",
    "            Rossmann_df = Rossmann[Rossmann['Store'] == store]\n",
    "            folder_path2=f'Store{store}/Correlation{corr_col}'\n",
    "            filename=f'Rossmann_{corr_col}_LSTM'\n",
    "            logs=f'logs_{corr_col}'\n",
    "\n",
    "            mk_dir(folder_path1,folder_path2)\n",
    "\n",
    "            df = Rossmann_df.drop(columns  = 'Date')\n",
    "            \n",
    "            features = list(corr_feat[corr_col].fillna(0))\n",
    "            features = [x for x in features if x != 0]\n",
    "            \n",
    "            train_df,train_sequences,test_sequences,train_size,train_dataset,test_dataset,scaler = pep_proc_summarizer(features,df)\n",
    "\n",
    "            PredictionModel, Predictor = def_model()\n",
    "            data_module = DataModule(train_sequences, test_sequences, batch_size = BATCH_SIZE)\n",
    "            trainer = def_trainer(folder_path1,folder_path2,filename,logs,store)\n",
    "            model = Predictor(n_features = len(train_df.columns))\n",
    "\n",
    "            iteration_start = time.monotonic()\n",
    "\n",
    "            trainer.fit(model, data_module)\n",
    "\n",
    "            iteration_end = time.monotonic()\n",
    "\n",
    "            Error_dic = {}\n",
    "            MASE=[]\n",
    "            RMSE=[]\n",
    "\n",
    "            for i in checkpoints:\n",
    "                predictions = recal_predict(folder_path1,folder_path2,filename,train_df.columns,i,test_dataset)\n",
    "                predictions_descaled = descale_(predictions)\n",
    "                csv_dataframe = pd.DataFrame(predictions_descaled, columns=[f'Rosmann{store}_epoch={i}'])\n",
    "                csv_dataframe.to_csv(f'{folder_path1}/{folder_path2}/Forecast/Rossmann{store}__epoch{i}.csv')\n",
    "                mase,rmse=calculate_errors(predictions_descaled,df,train_size)\n",
    "                MASE.append(mase)\n",
    "                RMSE.append(rmse)\n",
    "\n",
    "            Error_dic['MASE'] = MASE\n",
    "            Error_dic['RMSE'] = RMSE\n",
    "            Error_dic['TIME'] =iteration_end-iteration_start\n",
    "            Error_df = pd.DataFrame.from_dict(Error_dic)\n",
    "            Error_df.to_csv(f'{folder_path1}/{folder_path2}/Errors{corr_col}.csv')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28ed02c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%capture\n",
    "%tensorboard --logdir=./02-Results/LSTM/logs_correlation_th_=_0.05 --host localhost --port=3002\n",
    "%tensorboard --logdir=./02-Results/LSTM/logs_correlation_th_=_0.1 --host localhost --port=3003\n",
    "%tensorboard --logdir=./02-Results/LSTM/logs_correlation_th_=_0.15 --host localhost --port=3004\n",
    "%tensorboard --logdir=./02-Results/LSTM/logs_correlation_th_=_0.2 --host localhost --port=3005\n",
    "%tensorboard --logdir=./02-Results/LSTM/logs_correlation_th_=_0.25 --host localhost --port=3006"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c3d462fc",
   "metadata": {},
   "source": [
    "## LSTM Boruta-GB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "183d0fa0",
   "metadata": {},
   "outputs": [],
   "source": [
    "if Boruta_GB == True:\n",
    "    for store in tqdm(Rosmann_stores):\n",
    "        torch.manual_seed(42);\n",
    "        np.random.seed(42);\n",
    "        pl.seed_everything(42);  \n",
    "\n",
    "        Rossmann_df = Rossmann[Rossmann['Store'] == store]\n",
    "        \n",
    "        folder_path2=f'Store{store}/Boruta-GB'\n",
    "        filename=f'Rossmann_Boruta-GB_LSTM'\n",
    "        logs='logs_Boruta-GB'\n",
    "\n",
    "        mk_dir(folder_path1,folder_path2)\n",
    "        \n",
    "        df = Rossmann_df.drop(columns  = 'Date')\n",
    "        \n",
    "        BoGB_feat = pd.read_csv(f'02-Results/FS/Store{store}/Boruta-GB/borutaGB_df.csv')\n",
    "        features = list(BoGB_feat['important'].fillna(0))\n",
    "        features = [x for x in features if x != 0]\n",
    "        \n",
    "        train_df,train_sequences,test_sequences,train_size,train_dataset,test_dataset,scaler = pep_proc_summarizer(features,df)\n",
    "\n",
    "        PredictionModel, Predictor = def_model()\n",
    "        data_module = DataModule(train_sequences, test_sequences, batch_size = BATCH_SIZE)\n",
    "        trainer = def_trainer(folder_path1,folder_path2,filename,logs,store)\n",
    "        model = Predictor(n_features = len(train_df.columns))\n",
    "        \n",
    "\n",
    "        iteration_start = time.monotonic()\n",
    "\n",
    "        trainer.fit(model, data_module)\n",
    "        \n",
    "        iteration_end = time.monotonic()\n",
    "        \n",
    "        Error_dic = {}\n",
    "        MASE=[]\n",
    "        RMSE=[]\n",
    "\n",
    "        for i in checkpoints:\n",
    "            predictions = recal_predict(folder_path1,folder_path2,filename,train_df.columns,i,test_dataset)\n",
    "            predictions_descaled = descale_(predictions)\n",
    "            csv_dataframe = pd.DataFrame(predictions_descaled, columns=[f'Rosmann{store}_epoch={i}'])\n",
    "            csv_dataframe.to_csv(f'{folder_path1}/{folder_path2}/Forecast/Rossmann{store}_epoch{i}.csv')\n",
    "            mase,rmse=calculate_errors(predictions_descaled,df,train_size)\n",
    "            MASE.append(mase)\n",
    "            RMSE.append(rmse)\n",
    "\n",
    "        Error_dic['MASE'] = MASE\n",
    "        Error_dic['RMSE'] = RMSE\n",
    "        Error_dic['TIME'] =iteration_end-iteration_start\n",
    "        Error_df = pd.DataFrame.from_dict(Error_dic)\n",
    "        Error_df.to_csv(f'{folder_path1}/{folder_path2}/Errors.csv')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94922c77",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%capture\n",
    "%tensorboard --logdir=./02-Results/LSTM/logs_Boruta-GB --host localhost --port=3007"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7698c605",
   "metadata": {},
   "source": [
    "## LSTM Boruta-RF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95d14d9e",
   "metadata": {},
   "outputs": [],
   "source": [
    "if Boruta_RF == True:\n",
    "    for store in tqdm(Rosmann_stores):\n",
    "        torch.manual_seed(42);\n",
    "        np.random.seed(42);\n",
    "        pl.seed_everything(42);  \n",
    "\n",
    "        Rossmann_df = Rossmann[Rossmann['Store'] == store]\n",
    "        \n",
    "        folder_path2=f'Store{store}/Boruta-RF'\n",
    "        filename=f'Rossmann_Boruta-RF_LSTM'\n",
    "        logs='logs_Boruta-RF'\n",
    "\n",
    "        mk_dir(folder_path1,folder_path2)\n",
    "        \n",
    "        df = Rossmann_df.drop(columns  = 'Date')\n",
    "        \n",
    "        BoRF_feat = pd.read_csv(f'02-Results/FS/Store{store}/Boruta-RF/BorutaRF_df.csv')\n",
    "        features = list(BoRF_feat['important'].fillna(0))\n",
    "        features = [x for x in features if x != 0]\n",
    "        \n",
    "        train_df,train_sequences,test_sequences,train_size,train_dataset,test_dataset,scaler = pep_proc_summarizer(features,df)\n",
    "\n",
    "        PredictionModel, Predictor = def_model()\n",
    "        data_module = DataModule(train_sequences, test_sequences, batch_size = BATCH_SIZE)\n",
    "        trainer = def_trainer(folder_path1,folder_path2,filename,logs,store)\n",
    "        model = Predictor(n_features = len(train_df.columns))\n",
    "        \n",
    "\n",
    "        iteration_start = time.monotonic()\n",
    "\n",
    "        trainer.fit(model, data_module)\n",
    "        \n",
    "        iteration_end = time.monotonic()\n",
    "        \n",
    "        Error_dic = {}\n",
    "        MASE=[]\n",
    "        RMSE=[]\n",
    "\n",
    "        for i in checkpoints:\n",
    "            predictions = recal_predict(folder_path1,folder_path2,filename,train_df.columns,i,test_dataset)\n",
    "            predictions_descaled = descale_(predictions)\n",
    "            csv_dataframe = pd.DataFrame(predictions_descaled, columns=[f'Rosmann{store}_epoch={i}'])\n",
    "            csv_dataframe.to_csv(f'{folder_path1}/{folder_path2}/Forecast/Rossmann{store}_epoch{i}.csv')\n",
    "            mase,rmse=calculate_errors(predictions_descaled,df,train_size)\n",
    "            MASE.append(mase)\n",
    "            RMSE.append(rmse)\n",
    "\n",
    "        Error_dic['MASE'] = MASE\n",
    "        Error_dic['RMSE'] = RMSE\n",
    "        Error_dic['TIME'] =iteration_end-iteration_start\n",
    "        Error_df = pd.DataFrame.from_dict(Error_dic)\n",
    "        Error_df.to_csv(f'{folder_path1}/{folder_path2}/Errors.csv')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44c76124",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%capture\n",
    "%tensorboard --logdir=./02-Results/LSTM/logs_Boruta-RF --host localhost --port=3008"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20bc4caf",
   "metadata": {},
   "source": [
    "## LSTM BorutaSHAP-GB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4568e515",
   "metadata": {},
   "outputs": [],
   "source": [
    "if BorutaSHAP_GB == True:\n",
    "    for store in tqdm(Rosmann_stores):\n",
    "        torch.manual_seed(42);\n",
    "        np.random.seed(42);\n",
    "        pl.seed_everything(42);  \n",
    "\n",
    "        Rossmann_df = Rossmann[Rossmann['Store'] == store]\n",
    "        \n",
    "        folder_path2=f'Store{store}/BorutaSHAP-GB'\n",
    "        filename=f'Rossmann_BorutaSHAP-GB_LSTM'\n",
    "        logs='logs_BorutaSHAP-GB'\n",
    "\n",
    "        mk_dir(folder_path1,folder_path2)\n",
    "        \n",
    "        df = Rossmann_df.drop(columns  = 'Date')\n",
    "        \n",
    "        BoSHAPGB_feat = pd.read_csv(f'02-Results/FS/Store{store}/BorutaSHAP-GB/borutaSHAPGB_df.csv')\n",
    "        features = list(BoSHAPGB_feat['important'].fillna(0))\n",
    "        features = [x for x in features if x != 0]\n",
    "        \n",
    "        train_df,train_sequences,test_sequences,train_size,train_dataset,test_dataset,scaler = pep_proc_summarizer(features,df)\n",
    "\n",
    "        PredictionModel, Predictor = def_model()\n",
    "        data_module = DataModule(train_sequences, test_sequences, batch_size = BATCH_SIZE)\n",
    "        trainer = def_trainer(folder_path1,folder_path2,filename,logs,store)\n",
    "        model = Predictor(n_features = len(train_df.columns))\n",
    "        \n",
    "\n",
    "        iteration_start = time.monotonic()\n",
    "\n",
    "        trainer.fit(model, data_module)\n",
    "        \n",
    "        iteration_end = time.monotonic()\n",
    "        \n",
    "        Error_dic = {}\n",
    "        MASE=[]\n",
    "        RMSE=[]\n",
    "\n",
    "        for i in checkpoints:\n",
    "            predictions = recal_predict(folder_path1,folder_path2,filename,train_df.columns,i,test_dataset)\n",
    "            predictions_descaled = descale_(predictions)\n",
    "            csv_dataframe = pd.DataFrame(predictions_descaled, columns=[f'Rosmann{store}_epoch={i}'])\n",
    "            csv_dataframe.to_csv(f'{folder_path1}/{folder_path2}/Forecast/Rossmann{store}_epoch{i}.csv')\n",
    "            mase,rmse=calculate_errors(predictions_descaled,df,train_size)\n",
    "            MASE.append(mase)\n",
    "            RMSE.append(rmse)\n",
    "\n",
    "        Error_dic['MASE'] = MASE\n",
    "        Error_dic['RMSE'] = RMSE\n",
    "        Error_dic['TIME'] =iteration_end-iteration_start\n",
    "        Error_df = pd.DataFrame.from_dict(Error_dic)\n",
    "        Error_df.to_csv(f'{folder_path1}/{folder_path2}/Errors.csv')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e85cf383",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%capture\n",
    "%tensorboard --logdir=./02-Results/LSTM/logs_BorutaSHAP-GB --host localhost --port=3009"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a9a19cc0",
   "metadata": {},
   "source": [
    "## LSTM BorutaSHAP-RF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5fa222f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "if BorutaSHAP_RF == True:\n",
    "    for store in tqdm(Rosmann_stores):\n",
    "        torch.manual_seed(42);\n",
    "        np.random.seed(42);\n",
    "        pl.seed_everything(42);  \n",
    "\n",
    "        Rossmann_df = Rossmann[Rossmann['Store'] == store]\n",
    "        \n",
    "        folder_path2=f'Store{store}/BorutaSHAP-RF'\n",
    "        filename=f'Rossmann_BorutaSHAP-RF_LSTM'\n",
    "        logs='logs_BorutaSHAP-RF'\n",
    "\n",
    "        mk_dir(folder_path1,folder_path2)\n",
    "        \n",
    "        df = Rossmann_df.drop(columns  = 'Date')\n",
    "        \n",
    "        BoSHAPRF_feat = pd.read_csv(f'02-Results/FS/Store{store}/BorutaSHAP-RF/borutaSHAPRF_df.csv')\n",
    "        features = list(BoSHAPRF_feat['important'].fillna(0))\n",
    "        features = [x for x in features if x != 0]\n",
    "        \n",
    "        train_df,train_sequences,test_sequences,train_size,train_dataset,test_dataset,scaler = pep_proc_summarizer(features,df)\n",
    "\n",
    "        PredictionModel, Predictor = def_model()\n",
    "        data_module = DataModule(train_sequences, test_sequences, batch_size = BATCH_SIZE)\n",
    "        trainer = def_trainer(folder_path1,folder_path2,filename,logs,store)\n",
    "        model = Predictor(n_features = len(train_df.columns))\n",
    "        \n",
    "\n",
    "        iteration_start = time.monotonic()\n",
    "\n",
    "        trainer.fit(model, data_module)\n",
    "        \n",
    "        iteration_end = time.monotonic()\n",
    "        \n",
    "        Error_dic = {}\n",
    "        MASE=[]\n",
    "        RMSE=[]\n",
    "\n",
    "        for i in checkpoints:\n",
    "            predictions = recal_predict(folder_path1,folder_path2,filename,train_df.columns,i,test_dataset)\n",
    "            predictions_descaled = descale_(predictions)\n",
    "            csv_dataframe = pd.DataFrame(predictions_descaled, columns=[f'Rosmann{store}_epoch={i}'])\n",
    "            csv_dataframe.to_csv(f'{folder_path1}/{folder_path2}/Forecast/Rossmann{store}_epoch{i}.csv')\n",
    "            mase,rmse=calculate_errors(predictions_descaled,df,train_size)\n",
    "            MASE.append(mase)\n",
    "            RMSE.append(rmse)\n",
    "\n",
    "        Error_dic['MASE'] = MASE\n",
    "        Error_dic['RMSE'] = RMSE\n",
    "        Error_dic['TIME'] =iteration_end-iteration_start\n",
    "        Error_df = pd.DataFrame.from_dict(Error_dic)\n",
    "        Error_df.to_csv(f'{folder_path1}/{folder_path2}/Errors.csv')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e8e0aad",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%capture\n",
    "%tensorboard --logdir=./02-Results/LSTM/logs_BorutaSHAP-RF --host localhost --port=3010"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa9d8c42",
   "metadata": {},
   "source": [
    "## LSTM IMV-Tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb63a563",
   "metadata": {},
   "outputs": [],
   "source": [
    "if IMV_Tensor == True:\n",
    "    for store in tqdm(Rosmann_stores):\n",
    "        torch.manual_seed(42);\n",
    "        np.random.seed(42);\n",
    "        pl.seed_everything(42);  \n",
    "\n",
    "        Rossmann_df = Rossmann[Rossmann['Store'] == store]\n",
    "        \n",
    "        folder_path2=f'Store{store}/IMV_Tensor'\n",
    "        filename=f'Rossmann_IMV_Tensor_LSTM'\n",
    "        logs='logs_IMV_Tensor'\n",
    "\n",
    "        mk_dir(folder_path1,folder_path2)\n",
    "        df = Rossmann_df.drop(columns  = 'Date')\n",
    "        \n",
    "        IMV_Tensor_feats = pd.read_csv(f'02-Results/FS/Store{store}/IMV-LSTM/IMV_Tensor.csv')\n",
    "        features = list(IMV_Tensor_feats['features'].loc[IMV_Tensor_feats['Importance'] > 0.045])\n",
    "\n",
    "        \n",
    "        train_df,train_sequences,test_sequences,train_size,train_dataset,test_dataset,scaler = pep_proc_summarizer(features,df)\n",
    "        PredictionModel, Predictor = def_model()\n",
    "        data_module = DataModule(train_sequences, test_sequences, batch_size = BATCH_SIZE)\n",
    "        trainer = def_trainer(folder_path1,folder_path2,filename,logs,store)\n",
    "        model = Predictor(n_features = len(train_df.columns))\n",
    "        \n",
    "\n",
    "        iteration_start = time.monotonic()\n",
    "\n",
    "        trainer.fit(model, data_module)\n",
    "        \n",
    "        iteration_end = time.monotonic()\n",
    "        \n",
    "        Error_dic = {}\n",
    "        MASE=[]\n",
    "        RMSE=[]\n",
    "\n",
    "        for i in checkpoints:\n",
    "            predictions = recal_predict(folder_path1,folder_path2,filename,train_df.columns,i,test_dataset)\n",
    "            predictions_descaled = descale_(predictions)\n",
    "            csv_dataframe = pd.DataFrame(predictions_descaled, columns=[f'Rosmann{store}_epoch={i}'])\n",
    "            csv_dataframe.to_csv(f'{folder_path1}/{folder_path2}/Forecast/Rossmann{store}_epoch{i}.csv')\n",
    "            mase,rmse=calculate_errors(predictions_descaled,df,train_size)\n",
    "            MASE.append(mase)\n",
    "            RMSE.append(rmse)\n",
    "\n",
    "        Error_dic['MASE'] = MASE\n",
    "        Error_dic['RMSE'] = RMSE\n",
    "        Error_dic['TIME'] =iteration_end-iteration_start\n",
    "        Error_df = pd.DataFrame.from_dict(Error_dic)\n",
    "        Error_df.to_csv(f'{folder_path1}/{folder_path2}/Errors.csv')\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be941d28",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%capture\n",
    "%tensorboard --logdir=./02-Results/LSTM/logs_IMV_Tensor --host localhost --port=6012"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be7f2ce4",
   "metadata": {},
   "source": [
    "## LSTM IMV-Full"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f976b6a",
   "metadata": {},
   "outputs": [],
   "source": [
    "if IMV_Full == True:\n",
    "    for store in tqdm(Rosmann_stores):\n",
    "        torch.manual_seed(42);\n",
    "        np.random.seed(42);\n",
    "        pl.seed_everything(42);  \n",
    "\n",
    "        Rossmann_df = Rossmann[Rossmann['Store'] == store]\n",
    "        \n",
    "        folder_path2=f'Store{store}/IMV_Full'\n",
    "        filename=f'Rossmann_IMV_Full_LSTM'\n",
    "        logs='logs_IMV_Full'\n",
    "\n",
    "        mk_dir(folder_path1,folder_path2)\n",
    "        \n",
    "        df = Rossmann_df.drop(columns  = 'Date')\n",
    "        \n",
    "        IMV_Full_feats = pd.read_csv(f'02-Results/FS/Store{store}/IMV-LSTM/IMV_Full.csv')\n",
    "        features = list(IMV_Full_feats['features'].loc[IMV_Full_feats['Importance'] > 0.06])\n",
    "        \n",
    "        train_df,train_sequences,test_sequences,train_size,train_dataset,test_dataset,scaler = pep_proc_summarizer(features,df)\n",
    "\n",
    "        PredictionModel, Predictor = def_model()\n",
    "        data_module = DataModule(train_sequences, test_sequences, batch_size = BATCH_SIZE)\n",
    "        trainer = def_trainer(folder_path1,folder_path2,filename,logs,store)\n",
    "        model = Predictor(n_features = len(train_df.columns))\n",
    "        \n",
    "\n",
    "        iteration_start = time.monotonic()\n",
    "\n",
    "        trainer.fit(model, data_module)\n",
    "        \n",
    "        iteration_end = time.monotonic()\n",
    "        \n",
    "        Error_dic = {}\n",
    "        MASE=[]\n",
    "        RMSE=[]\n",
    "\n",
    "        for i in checkpoints:\n",
    "            predictions = recal_predict(folder_path1,folder_path2,filename,train_df.columns,i,test_dataset)\n",
    "            predictions_descaled = descale_(predictions)\n",
    "            csv_dataframe = pd.DataFrame(predictions_descaled, columns=[f'Rosmann{store}_epoch={i}'])\n",
    "            csv_dataframe.to_csv(f'{folder_path1}/{folder_path2}/Forecast/Rossmann{store}_epoch{i}.csv')\n",
    "            mase,rmse=calculate_errors(predictions_descaled,df,train_size)\n",
    "            MASE.append(mase)\n",
    "            RMSE.append(rmse)\n",
    "\n",
    "        Error_dic['MASE'] = MASE\n",
    "        Error_dic['RMSE'] = RMSE\n",
    "        Error_dic['TIME'] =iteration_end-iteration_start\n",
    "        Error_df = pd.DataFrame.from_dict(Error_dic)\n",
    "        Error_df.to_csv(f'{folder_path1}/{folder_path2}/Errors.csv')\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9fe9e67",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%capture\n",
    "%tensorboard --logdir=./02-Results/LSTM/logs_LIME-LSTM --host localhost --port=3012"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9cda9f07",
   "metadata": {},
   "source": [
    "## LSTM LIME-LSTM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "761cacdd",
   "metadata": {},
   "outputs": [],
   "source": [
    "if LIME_train == True:\n",
    "    LIME_inst_th = [0.007,0.008,0.009,0.01]\n",
    "    for threshold in LIME_inst_th:\n",
    "        for store in tqdm(Rosmann_stores):        \n",
    "            \n",
    "            inst_LSTMLIME = pd.read_csv(f'02-Results/FS/Store{store}/LIME-LSTM/LIME-LSTM.csv')\n",
    "\n",
    "            torch.manual_seed(42);\n",
    "            np.random.seed(42);\n",
    "            pl.seed_everything(42);  \n",
    "\n",
    "            Rossmann_df = Rossmann[Rossmann['Store'] == store]\n",
    "            folder_path2=f'Store{store}/LIME={threshold}'\n",
    "            filename=f'Rossmann_LIME={threshold}_LSTM'\n",
    "            logs=f'logs_LIME={threshold}'\n",
    "\n",
    "            mk_dir(folder_path1,folder_path2)\n",
    "\n",
    "            df = Rossmann_df.drop(columns  = 'Date')\n",
    "            \n",
    "            features = list(inst_LSTMLIME['Features'].loc[inst_LSTMLIME['LIME_value'] > threshold])\n",
    "            \n",
    "            train_df,train_sequences,test_sequences,train_size,train_dataset,test_dataset,scaler = pep_proc_summarizer(features,df)\n",
    "\n",
    "            PredictionModel, Predictor = def_model()\n",
    "            data_module = DataModule(train_sequences, test_sequences, batch_size = BATCH_SIZE)\n",
    "            trainer = def_trainer(folder_path1,folder_path2,filename,logs,store)\n",
    "            model = Predictor(n_features = len(train_df.columns))\n",
    "\n",
    "            iteration_start = time.monotonic()\n",
    "\n",
    "            trainer.fit(model, data_module)\n",
    "\n",
    "            iteration_end = time.monotonic()\n",
    "\n",
    "            Error_dic = {}\n",
    "            MASE=[]\n",
    "            RMSE=[]\n",
    "\n",
    "            for i in checkpoints:\n",
    "                predictions = recal_predict(folder_path1,folder_path2,filename,train_df.columns,i,test_dataset)\n",
    "                predictions_descaled = descale_(predictions)\n",
    "                csv_dataframe = pd.DataFrame(predictions_descaled, columns=[f'Rosmann{store}_epoch={i}'])\n",
    "                csv_dataframe.to_csv(f'{folder_path1}/{folder_path2}/Forecast/Rossmann{store}__epoch{i}.csv')\n",
    "                mase,rmse=calculate_errors(predictions_descaled,df,train_size)\n",
    "                MASE.append(mase)\n",
    "                RMSE.append(rmse)\n",
    "\n",
    "            Error_dic['MASE'] = MASE\n",
    "            Error_dic['RMSE'] = RMSE\n",
    "            Error_dic['TIME'] =iteration_end-iteration_start\n",
    "            Error_df = pd.DataFrame.from_dict(Error_dic)\n",
    "            Error_df.to_csv(f'{folder_path1}/{folder_path2}/Errors{threshold}.csv')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3a37ab7",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%capture\n",
    "%tensorboard --logdir=./02-Results/LSTM/logs_LIME=0.007 --host localhost --port=3013\n",
    "%tensorboard --logdir=./02-Results/LSTM/logs_LIME=0.008 --host localhost --port=3014\n",
    "%tensorboard --logdir=./02-Results/LSTM/logs_LIME=0.009 --host localhost --port=3015\n",
    "%tensorboard --logdir=./02-Results/LSTM/logs_LIME=0.01 --host localhost --port=3016"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5fdb32e6",
   "metadata": {},
   "source": [
    "## LSTM Instance SHAP-LSTM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9427a2f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "if SHAP_insta == True:\n",
    "    SHAP_inst_th = [0.05,0.1,0.15,0.2]\n",
    "    for threshold in SHAP_inst_th:\n",
    "        for store in tqdm(Rosmann_stores):        \n",
    "            \n",
    "            inst_SHAPLSTM = pd.read_csv(f'02-Results/FS/Store{store}/SHAP-LSTM/inst_SHAP_LSTM.csv')\n",
    "\n",
    "            torch.manual_seed(42);\n",
    "            np.random.seed(42);\n",
    "            pl.seed_everything(42);  \n",
    "\n",
    "            Rossmann_df = Rossmann[Rossmann['Store'] == store]\n",
    "            folder_path2=f'Store{store}/instSHAP={threshold}'\n",
    "            filename=f'Rossmann_instSHAP={threshold}_LSTM'\n",
    "            logs=f'logs_instSHAP={threshold}'\n",
    "\n",
    "            mk_dir(folder_path1,folder_path2)\n",
    "\n",
    "            df = Rossmann_df.drop(columns  = 'Date')\n",
    "            \n",
    "            features = list(inst_SHAPLSTM['feature_name'].loc[inst_SHAPLSTM['max_shap_value'] > threshold])\n",
    "            \n",
    "            train_df,train_sequences,test_sequences,train_size,train_dataset,test_dataset,scaler = pep_proc_summarizer(features,df)\n",
    "\n",
    "            PredictionModel, Predictor = def_model()\n",
    "            data_module = DataModule(train_sequences, test_sequences, batch_size = BATCH_SIZE)\n",
    "            trainer = def_trainer(folder_path1,folder_path2,filename,logs,store)\n",
    "            model = Predictor(n_features = len(train_df.columns))\n",
    "\n",
    "            iteration_start = time.monotonic()\n",
    "\n",
    "            trainer.fit(model, data_module)\n",
    "\n",
    "            iteration_end = time.monotonic()\n",
    "\n",
    "            Error_dic = {}\n",
    "            MASE=[]\n",
    "            RMSE=[]\n",
    "\n",
    "            for i in checkpoints:\n",
    "                predictions = recal_predict(folder_path1,folder_path2,filename,train_df.columns,i,test_dataset)\n",
    "                predictions_descaled = descale_(predictions)\n",
    "                csv_dataframe = pd.DataFrame(predictions_descaled, columns=[f'Rosmann{store}_epoch={i}'])\n",
    "                csv_dataframe.to_csv(f'{folder_path1}/{folder_path2}/Forecast/Rossmann{store}__epoch{i}.csv')\n",
    "                mase,rmse=calculate_errors(predictions_descaled,df,train_size)\n",
    "                MASE.append(mase)\n",
    "                RMSE.append(rmse)\n",
    "\n",
    "            Error_dic['MASE'] = MASE\n",
    "            Error_dic['RMSE'] = RMSE\n",
    "            Error_dic['TIME'] =iteration_end-iteration_start\n",
    "            Error_df = pd.DataFrame.from_dict(Error_dic)\n",
    "            Error_df.to_csv(f'{folder_path1}/{folder_path2}/Errors{threshold}.csv')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "510a6909",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%capture\n",
    "%tensorboard --logdir=./02-Results/LSTM/logs_instSHAP=0.05 --host localhost --port=1017\n",
    "%tensorboard --logdir=./02-Results/LSTM/logs_instSHAP=0.1 --host localhost --port=1018\n",
    "%tensorboard --logdir=./02-Results/LSTM/logs_instSHAP=0.15 --host localhost --port=1019\n",
    "%tensorboard --logdir=./02-Results/LSTM/logs_instSHAP=0.2 --host localhost --port=1020"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e6bfce28",
   "metadata": {},
   "source": [
    "## LSTM Average SHAP-LSTM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78ee5022",
   "metadata": {},
   "outputs": [],
   "source": [
    "if SHAP_avrag == True:\n",
    "    SHAP_avg_th = [0.01,0.02,0.03,0.04]\n",
    "    for threshold in SHAP_avg_th:\n",
    "        for store in tqdm(Rosmann_stores):        \n",
    "            \n",
    "            avg_SHAPLSTM = pd.read_csv(f'02-Results/FS/Store{store}/SHAP-LSTM/avg_SHAP_LSTM.csv')\n",
    "\n",
    "            torch.manual_seed(42);\n",
    "            np.random.seed(42);\n",
    "            pl.seed_everything(42);  \n",
    "\n",
    "            Rossmann_df = Rossmann[Rossmann['Store'] == store]\n",
    "            folder_path2=f'Store{store}/avgSHAP={threshold}'\n",
    "            filename=f'Rossmann_avgSHAP={threshold}_LSTM'\n",
    "            logs=f'logs_avgSHAP={threshold}'\n",
    "\n",
    "            mk_dir(folder_path1,folder_path2)\n",
    "\n",
    "            df = Rossmann_df.drop(columns  = 'Date')\n",
    "            \n",
    "            features = list(avg_SHAPLSTM['Unnamed: 0'].loc[avg_SHAPLSTM['Importance'] > threshold])\n",
    "            \n",
    "            train_df,train_sequences,test_sequences,train_size,train_dataset,test_dataset,scaler = pep_proc_summarizer(features,df)\n",
    "\n",
    "            PredictionModel, Predictor = def_model()\n",
    "            data_module = DataModule(train_sequences, test_sequences, batch_size = BATCH_SIZE)\n",
    "            trainer = def_trainer(folder_path1,folder_path2,filename,logs,store)\n",
    "            model = Predictor(n_features = len(train_df.columns))\n",
    "\n",
    "            iteration_start = time.monotonic()\n",
    "\n",
    "            trainer.fit(model, data_module)\n",
    "\n",
    "            iteration_end = time.monotonic()\n",
    "\n",
    "            Error_dic = {}\n",
    "            MASE=[]\n",
    "            RMSE=[]\n",
    "\n",
    "            for i in checkpoints:\n",
    "                predictions = recal_predict(folder_path1,folder_path2,filename,train_df.columns,i,test_dataset)\n",
    "                predictions_descaled = descale_(predictions)\n",
    "                csv_dataframe = pd.DataFrame(predictions_descaled, columns=[f'Rosmann{store}_epoch={i}'])\n",
    "                csv_dataframe.to_csv(f'{folder_path1}/{folder_path2}/Forecast/Rossmann{store}__epoch{i}.csv')\n",
    "                mase,rmse=calculate_errors(predictions_descaled,df,train_size)\n",
    "                MASE.append(mase)\n",
    "                RMSE.append(rmse)\n",
    "\n",
    "            Error_dic['MASE'] = MASE\n",
    "            Error_dic['RMSE'] = RMSE\n",
    "            Error_dic['TIME'] =iteration_end-iteration_start\n",
    "            Error_df = pd.DataFrame.from_dict(Error_dic)\n",
    "            Error_df.to_csv(f'{folder_path1}/{folder_path2}/Errors{threshold}.csv')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb8530b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%capture\n",
    "%tensorboard --logdir=./02-Results/LSTM/logs_avgSHAP=0.01 --host localhost --port=3021\n",
    "%tensorboard --logdir=./02-Results/LSTM/logs_avgSHAP=0.02 --host localhost --port=3022\n",
    "%tensorboard --logdir=./02-Results/LSTM/logs_avgSHAP=0.03 --host localhost --port=3023\n",
    "%tensorboard --logdir=./02-Results/LSTM/logs_avgSHAP=0.04 --host localhost --port=3024"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "afb5177a",
   "metadata": {},
   "outputs": [],
   "source": [
    "%tensorboard dev upload --logdir './02-Results/LSTM/logs_univariate' --one_shot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04da7600",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "celltoolbar": "Raw Cell Format",
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
