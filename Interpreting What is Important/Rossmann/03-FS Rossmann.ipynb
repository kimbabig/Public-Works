{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a199d563",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd   #preprocessing\n",
    "import numpy as np\n",
    "import math\n",
    "from tqdm.notebook import tqdm\n",
    "import pytorch_lightning as pl\n",
    "from pytorch_lightning.callbacks import ModelCheckpoint, EarlyStopping\n",
    "from pytorch_lightning.loggers import TensorBoardLogger\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "import matplotlib.pyplot as plt    \n",
    "import torch          #modelling\n",
    "import torch.autograd as autograd\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader,TensorDataset\n",
    "from collections import defaultdict\n",
    "\n",
    "\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.ensemble import GradientBoostingRegressor\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error\n",
    "from IMV_LSTM import IMVTensorLSTM\n",
    "from IMV_LSTM import IMVFullLSTM\n",
    "from boruta import BorutaPy\n",
    "from BorutaShap import BorutaShap\n",
    "import shap\n",
    "import shap.plots\n",
    "\n",
    "\n",
    "#import operator\n",
    "import time\n",
    "import os\n",
    "import operator\n",
    "\n",
    "from IPython.utils import io\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\", \".*does not have many workers.*\")\n",
    "warnings.filterwarnings(\"ignore\", category=DeprecationWarning) \n",
    "warnings.filterwarnings(\"ignore\", category=UserWarning) \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7b8c76f",
   "metadata": {},
   "outputs": [],
   "source": [
    "N_EPOCHS = 1001\n",
    "BATCH_SIZE = 7\n",
    "SEQUENCE_LENGTH = 15\n",
    "N_HIDDEN = 32\n",
    "N_LAYERS = 1\n",
    "PATIENCE = 50\n",
    "LEARNING = 0.001\n",
    "\n",
    "folder_path1='02-Results/FS'\n",
    "TGT = 'Sales'\n",
    "\n",
    "Correlation = False\n",
    "\n",
    "BorutaGB = False\n",
    "BorutaRF = False\n",
    "\n",
    "BorutaSHAPGB = False\n",
    "BorutaSHAPRF = False\n",
    "\n",
    "LSTM_SHAP = False\n",
    "LSTM_LIME_ = True\n",
    "\n",
    "IMV_Tensor = False\n",
    "IMV_Full = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db9100fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Dataset(Dataset):\n",
    "    def __init__(self,sequences):\n",
    "        self.sequences = sequences\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.sequences)\n",
    "\n",
    "    def __getitem__(self,idx):\n",
    "        sequence, label = self.sequences[idx]\n",
    "\n",
    "        return dict(sequence = torch.Tensor(sequence.to_numpy()),\n",
    "            label = torch.tensor(label).float())\n",
    "\n",
    "class DataModule(pl.LightningDataModule):\n",
    "    def __init__(self, train_seqeunces,test_sequences, batch_size=8):\n",
    "        super().__init__()\n",
    "        self.train_sequences = train_sequences\n",
    "        self.test_sequences = test_sequences\n",
    "        self.batch_size = batch_size\n",
    "\n",
    "    def setup(self,stage=None):\n",
    "        self.train_dataset = Dataset(self.train_sequences)\n",
    "        self.test_dataset = Dataset(self.test_sequences)\n",
    "\n",
    "    def train_dataloader(self):\n",
    "        return DataLoader(\n",
    "            self.train_dataset,\n",
    "            batch_size = self.batch_size,\n",
    "            shuffle = False,\n",
    "            num_workers = 0)\n",
    "\n",
    "    def val_dataloader(self):\n",
    "        return DataLoader(\n",
    "            self.test_dataset,\n",
    "            batch_size = 1,\n",
    "            shuffle = False,\n",
    "            num_workers = 0)\n",
    "    \n",
    "    \n",
    "class PredictionModel(nn.Module):\n",
    "    def __init__(self, n_features, n_hidden = N_HIDDEN, n_layers = N_LAYERS):\n",
    "        super().__init__()\n",
    "        self.n_hidden = n_hidden\n",
    "        self.lstm = nn.LSTM(\n",
    "            input_size = n_features,\n",
    "            hidden_size = n_hidden,\n",
    "            batch_first = True,\n",
    "            num_layers = n_layers,\n",
    "            dropout = 0.2)\n",
    "        self.regressor = nn.Linear(n_hidden,1)\n",
    "\n",
    "    def forward(self,x):\n",
    "        self.lstm.flatten_parameters()\n",
    "        _, (hidden, _) = self.lstm(x)\n",
    "        out = hidden[-1]\n",
    "        return self.regressor(out)\n",
    "\n",
    "\n",
    "class Predictor(pl.LightningModule):\n",
    "    def __init__(self, n_features: int):\n",
    "        super().__init__()\n",
    "        self.model=PredictionModel(n_features)\n",
    "        self.criterion = nn.MSELoss()\n",
    "\n",
    "    def forward(self, x, labels= None):\n",
    "        output = self.model(x)\n",
    "        loss = 0\n",
    "        if labels is not None:\n",
    "            loss = self.criterion(output, labels.unsqueeze(dim=1))\n",
    "        return loss, output\n",
    "\n",
    "    def training_step(self, batch, batch_index):\n",
    "        sequences = batch['sequence']\n",
    "        labels = batch['label']\n",
    "        loss, outputs = self(sequences, labels)\n",
    "        self.log('train_loss', loss, prog_bar = True, logger=False)\n",
    "        return loss\n",
    "\n",
    "    def validation_step(self, batch, batch_index):\n",
    "        sequences = batch['sequence']\n",
    "        labels = batch['label']\n",
    "        loss, outputs = self(sequences, labels)\n",
    "        self.log('val_loss', loss, prog_bar = True, logger=False)\n",
    "        return loss\n",
    "\n",
    "    def test_step(self, batch, batch_index):\n",
    "        sequences = batch['sequence']\n",
    "        labels = batch['label']\n",
    "        loss, outputs = self(sequences, labels)\n",
    "        self.log('test_loss', loss, prog_bar = True, logger=False)\n",
    "        return loss\n",
    "\n",
    "    def configure_optimizers(self):\n",
    "        return optim.AdamW(self.parameters(), lr = LEARNING)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64478159",
   "metadata": {},
   "outputs": [],
   "source": [
    "def features_dataframe(df,features):\n",
    "    rows = []\n",
    "\n",
    "    for _, row in df.iterrows():\n",
    "        row_data = dict(\n",
    "            Sales = df[f'{TGT}'],\n",
    "        )\n",
    "        for column in features:\n",
    "            row_data[column] = row[column]\n",
    "            \n",
    "        rows.append(row_data)\n",
    "    \n",
    "    features_df = pd.DataFrame(rows)\n",
    "\n",
    "    return features_df\n",
    "\n",
    "#spliits the data in test and train\n",
    "def train_test_spliter(ratio,features_df ):\n",
    "    train_size = int(len(features_df)-ratio)\n",
    "    train_df, test_df = features_df[:train_size], features_df[train_size + 1:]\n",
    "\n",
    "    return train_df, test_df, train_size\n",
    "\n",
    "def data_scaler(train_df,test_df):\n",
    "    scaler = MinMaxScaler(feature_range=(-1,1))\n",
    "    scaler = scaler.fit(train_df)\n",
    "    \n",
    "    train_df = pd.DataFrame(\n",
    "        scaler.transform(train_df),\n",
    "        index = train_df.index,\n",
    "        columns = train_df.columns)\n",
    "\n",
    "    test_df = pd.DataFrame(\n",
    "        scaler.transform(test_df),\n",
    "        index = test_df.index,\n",
    "        columns = test_df.columns)\n",
    "    \n",
    "    return train_df, test_df, scaler\n",
    "\n",
    "\n",
    "def create_sequences (input_data:pd.DataFrame, target_column, sequence_length):\n",
    "    sequences = []\n",
    "    data_size = len(input_data)\n",
    "\n",
    "    for i in (range(data_size - sequence_length)):\n",
    "        sequence = input_data[i:i+sequence_length]\n",
    "        label_position = i + sequence_length\n",
    "        label = input_data.iloc[label_position][target_column]\n",
    "        sequences.append((sequence,label))\n",
    "    \n",
    "    return sequences\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ac9e0be",
   "metadata": {},
   "outputs": [],
   "source": [
    "def pep_proc_summarizer(features,df):\n",
    "    features_df = features_dataframe(df,features) \n",
    "    #returns dataframe with the features to be analised\n",
    "    \n",
    "    #split into test and train and minmaxscaler\n",
    "    train_df, test_df, train_size =  train_test_spliter(105,features_df)\n",
    "    train_df, test_df, scaler = data_scaler(train_df,test_df)\n",
    "    \n",
    "    #make sequences with the data\n",
    "    train_sequences = create_sequences(train_df,TGT,SEQUENCE_LENGTH)\n",
    "    test_sequences = create_sequences (test_df,TGT,SEQUENCE_LENGTH)\n",
    "    train_dataset = Dataset(train_sequences)\n",
    "    test_dataset = Dataset(test_sequences)\n",
    "    \n",
    "    return (train_df,train_sequences,test_sequences,train_size,train_dataset,test_dataset,scaler)\n",
    "\n",
    "def dict_to_dataframe(dict_list,seq_size):\n",
    "    column_names = []\n",
    "    data = []\n",
    "    for d in dict_list:\n",
    "        features = d['sequence'].numpy().flatten()\n",
    "        label = d['label'].numpy()\n",
    "        data.append(np.concatenate((features, label), axis=None))\n",
    "        \n",
    "    df = pd.DataFrame(data)\n",
    "    for j in range(seq_size):\n",
    "        if seq_size-j >= 10:\n",
    "            day_column_names = str(seq_size-j) + 'day_back'+'_' + train_df.columns\n",
    "            column_names.extend(day_column_names)\n",
    "        else:\n",
    "            day_column_names = '0' + str(seq_size-j) + 'day_back'+'_' + train_df.columns\n",
    "            column_names.extend(day_column_names)\n",
    "        \n",
    "    cols = [column_names + ['target']]\n",
    "    df.columns = cols\n",
    "    return df\n",
    "\n",
    "def dict_to_dataframeBoSHAP(dict_list,seq_size):\n",
    "    column_names = []\n",
    "    data = []\n",
    "    for d in dict_list:\n",
    "        features = d['sequence'].numpy().flatten()\n",
    "        label = d['label'].numpy()\n",
    "        data.append(np.concatenate((features, label), axis=None))\n",
    "        \n",
    "    df = pd.DataFrame(data)\n",
    "    for j in range(seq_size):\n",
    "        if seq_size-j >= 10:\n",
    "            day_column_names = str(seq_size-j) + 'day_back'+'_' + train_df.columns\n",
    "            column_names.extend(day_column_names)\n",
    "        else:\n",
    "            day_column_names = '0' + str(seq_size-j) + 'day_back'+'_' + train_df.columns\n",
    "            column_names.extend(day_column_names)\n",
    "        \n",
    "    column_names.append('target')\n",
    "    df.columns = column_names\n",
    "    return df\n",
    "\n",
    "def LSTM_predict(array):\n",
    "    with torch.no_grad():\n",
    "\n",
    "        predictions_lime= list(array[:SEQUENCE_LENGTH,0])\n",
    "        label_lime=[]\n",
    "\n",
    "        train_df = pd.DataFrame(array)\n",
    "        #features_df.columns = features\n",
    "\n",
    "        train_sequences = create_sequences(train_df,0,SEQUENCE_LENGTH)\n",
    "\n",
    "        data_module = DataModule(train_sequences, test_sequences, batch_size = BATCH_SIZE)\n",
    "\n",
    "        train_dataset = Dataset(train_sequences)\n",
    "\n",
    "        data_module.setup()\n",
    "\n",
    "        for item in train_dataset:\n",
    "            sequence = item['sequence']\n",
    "            label = item['label']\n",
    "            _,output = trained_model(sequence.unsqueeze(dim=0))\n",
    "            predictions_lime.append(output.item())\n",
    "            label_lime.append(label.item())\n",
    "\n",
    "        output = np.array(predictions_lime)\n",
    "\n",
    "    return output\n",
    "\n",
    "\n",
    "def LIME_Explainer(dates):\n",
    "    features_evo = {col: [] for col in train_df.columns}\n",
    "    \n",
    "    for local in tqdm(range(dates,0,-1)):\n",
    "        lst = explainer.explainer.explain_instance(train_df.iloc[-local],LSTM_predict).as_list()\n",
    "\n",
    "        result = [(tup[0].split()) for tup in lst]\n",
    "\n",
    "        for i in range(len(result)):\n",
    "            if len(result[i]) == 3:\n",
    "                features_evo[(train_df.columns[int(result[i][0])])].append(lst[i][1])\n",
    "            else:\n",
    "                features_evo[(train_df.columns[int(result[i][2])])].append(lst[i][1])\n",
    "\n",
    "        for key in features_evo.keys():\n",
    "            if len(features_evo[key]) < (dates - local)+1:\n",
    "                features_evo[key].append(0)\n",
    "                \n",
    "    return features_evo\n",
    "\n",
    "\n",
    "def return_shap_features(shap_values,train_df, number_to_return):\n",
    "    shapdict = {}\n",
    "    for k in train_df.columns:\n",
    "        shapdict[k] = 0\n",
    "        shapdict[f'{k}_count'] = 0\n",
    "\n",
    "\n",
    "    for i in shap_values:\n",
    "        for f in range(len(train_df.columns)):\n",
    "            shapdict[train_df.columns[f]] += abs(i[f])\n",
    "            shapdict[f'{train_df.columns[f]}_count'] += 1\n",
    "\n",
    "    avg_n0_shap = {}\n",
    "\n",
    "    for key in range(0,len(shapdict.keys()),2):\n",
    "        if shapdict[list(shapdict.keys())[key+1]] != 0 and shapdict[list(shapdict.keys())[key]] != 0:\n",
    "            avg_n0_shap[list(shapdict.keys())[key]] = shapdict[list(shapdict.keys())[key]]/shapdict[list(shapdict.keys())[key+1]]\n",
    "\n",
    "    sorted_avg_shap = dict(sorted(avg_n0_shap.items(), key=operator.itemgetter(1), reverse=True))\n",
    "    \n",
    "    top_features = dict(sorted(avg_n0_shap.items(), key=lambda item: item[1], reverse=True)[:number_to_return])\n",
    "    \n",
    "    return sorted_avg_shap, top_features\n",
    "\n",
    "\n",
    "    \n",
    "def return_shap_max(shap_values,train_df):\n",
    "    shapdict = {}\n",
    "    for k in train_df.columns:\n",
    "        shapdict[k] = []\n",
    "\n",
    "    for i in shap_values:\n",
    "        for f in range(len(train_df.columns)):\n",
    "            shapdict[train_df.columns[f]].append(abs(i[f]))\n",
    "\n",
    "    max_shap = {}\n",
    "    for key in shapdict.keys():\n",
    "        max_shap[key] = max(shapdict[key])\n",
    "\n",
    "    max_shap_df = pd.DataFrame.from_dict(max_shap, orient='index', columns=['max_shap_value'])\n",
    "    max_shap_df.index.name = 'feature_name'\n",
    "    max_shap_df = max_shap_df.sort_values(by='max_shap_value', ascending=False)\n",
    "\n",
    "    return max_shap_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "967d46dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "Rossmann = pd.read_csv('01-Data/Rossmann_treated.csv')\n",
    "Rosmann_stores = [23,64,103,133,135,216,224,256,266,311,355,405,455,487,558,649,672,714,738,742,773,785,\n",
    "                 830,870,880,899,937,978,1011,1069]\n",
    "Rossmann.drop(columns = 'Customers', inplace = True)\n",
    "Rossmann.drop(columns = 'Date', inplace = True)\n",
    "\n",
    "try:\n",
    "    os.mkdir(f'{folder_path1}')\n",
    "except:\n",
    "    pass\n",
    "\n",
    "for store in tqdm(Rosmann_stores):\n",
    "    try:\n",
    "        os.mkdir(f'{folder_path1}/Store{store}')\n",
    "    except:\n",
    "        pass"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ecce452",
   "metadata": {},
   "source": [
    "## PCC Feature Selection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e548487",
   "metadata": {},
   "outputs": [],
   "source": [
    "if Correlation == True:\n",
    "    corr_values = [0.05,0.10,0.15,0.20,0.25]\n",
    "    run_time = []\n",
    "    durations={}\n",
    "    \n",
    "    for store in tqdm(Rosmann_stores):            \n",
    "        folder_path2=f'Store{store}/Correlation'\n",
    "        try:\n",
    "            os.mkdir(f'{folder_path1}/{folder_path2}')\n",
    "        except:\n",
    "            pass\n",
    "    \n",
    "        Rossmann_df = Rossmann[Rossmann['Store'] == store]\n",
    "        correlation_df = pd.DataFrame(list(Rossmann_df.columns),columns = ['all_features'])\n",
    "        \n",
    "        \n",
    "        iteration_start = time.monotonic()\n",
    "        for cor_th in corr_values:  \n",
    "            Tgt_corr = Rossmann_df.corr()[abs(Rossmann_df.corr()) > cor_th]\n",
    "            features = Tgt_corr[Tgt_corr[TGT].notnull()].index\n",
    "            df = pd.DataFrame(features, columns = [f'correlation_th_=_{cor_th}'])\n",
    "            correlation_df = correlation_df.join(df,how='left')\n",
    "        iteration_end = time.monotonic()\n",
    "        correlation_df.to_csv(f\"{folder_path1}/{folder_path2}/correlation_df.csv\",index = False)\n",
    "        \n",
    "        run_time.append(iteration_end - iteration_start)\n",
    "        \n",
    "    durations['CompCost'] = run_time\n",
    "    duration_df =  pd.DataFrame.from_dict(durations)\n",
    "    duration_df.to_csv(f\"02-Results/Correlation_time.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d0df6b9",
   "metadata": {},
   "source": [
    "## Boruta-GB Feature Selection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d55c063",
   "metadata": {},
   "outputs": [],
   "source": [
    "if BorutaGB == True:    \n",
    "    durations={}\n",
    "    run_time = []\n",
    "    for store in tqdm(Rosmann_stores):\n",
    "        torch.manual_seed(42);\n",
    "        np.random.seed(42);\n",
    "        pl.seed_everything(42);\n",
    "        \n",
    "        folder_path2=f'Store{store}/Boruta-GB'\n",
    "        \n",
    "        try:\n",
    "            os.mkdir(f'{folder_path1}/{folder_path2}')\n",
    "        except:\n",
    "            pass\n",
    "        \n",
    "        Rossmann_df = Rossmann[Rossmann['Store'] == store]\n",
    "        estimator = GradientBoostingRegressor(n_estimators=100,\n",
    "                                              max_depth=3,\n",
    "                                              learning_rate=0.1,\n",
    "                                              loss='squared_error',\n",
    "                                              criterion='friedman_mse',\n",
    "                                              min_samples_split=2,\n",
    "                                              min_samples_leaf=1)\n",
    "        \n",
    "        boruta = BorutaPy(estimator = estimator,n_estimators = 30, max_iter =100)\n",
    "        \n",
    "        \n",
    "        features  = list(Rossmann_df.columns)\n",
    "        train_df,train_sequences,test_sequences,train_size,train_dataset,test_dataset,scaler=pep_proc_summarizer(features,Rossmann_df)\n",
    "        data = dict_to_dataframe(train_dataset, SEQUENCE_LENGTH)\n",
    "\n",
    "\n",
    "        iteration_start = time.monotonic()\n",
    "        boruta.fit(np.array(data.iloc[:,:-1]),np.array(data.iloc[:,-1]))\n",
    "        iteration_end = time.monotonic()\n",
    "\n",
    "        important = list(data.iloc[:,:-1].columns[boruta.support_])\n",
    "        important = [i[0][11:] for i in important]\n",
    "        important = list(set(important)) \n",
    "\n",
    "        unimportant = list(data.iloc[:,:-1].columns[boruta.support_weak_])\n",
    "        unimportant = [i[0][11:] for i in unimportant]\n",
    "        unimportant = list(set(unimportant))\n",
    "\n",
    "        tentative = list(data.iloc[:,:-1].columns[~(boruta.support_|boruta.support_weak_)])\n",
    "        tentative = [i[0][11:] for i in tentative]\n",
    "        tentative = list(set(tentative)) \n",
    "\n",
    "        for i in important:\n",
    "            if i in unimportant:\n",
    "                unimportant.remove(i)\n",
    "        for i in important:\n",
    "            if i in tentative:\n",
    "                tentative.remove(i)\n",
    "        for i in unimportant:\n",
    "            if i in tentative:\n",
    "                tentative.remove(i)\n",
    "\n",
    "        important_df = pd.DataFrame(important, columns=[f'important'])\n",
    "        unimportant_df = pd.DataFrame(unimportant, columns=[f'unimportant'])\n",
    "        tentative_df = pd.DataFrame(tentative, columns=[f'tentative'])\n",
    "        borutaGB_df = pd.DataFrame(list(Rossmann_df.columns),columns = ['all_features'])\n",
    "        borutaGB_df = borutaGB_df.join([important_df,unimportant_df, tentative_df],how='left')\n",
    "        borutaGB_df.to_csv(f\"{folder_path1}/{folder_path2}/borutaGB_df.csv\",index = False)\n",
    "\n",
    "        run_time.append(iteration_end - iteration_start)\n",
    "        \n",
    "    durations['CompCost'] = run_time\n",
    "    duration_df =  pd.DataFrame.from_dict(durations)\n",
    "    duration_df.to_csv(f\"02-Results/Boruta-GB_time.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e5824f51",
   "metadata": {},
   "source": [
    "## Boruta-RF Feature Selection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4d91d45",
   "metadata": {},
   "outputs": [],
   "source": [
    "if BorutaRF == True:  \n",
    "    durations={}\n",
    "    run_time = []\n",
    "    for store in tqdm(Rosmann_stores):\n",
    "        torch.manual_seed(42);\n",
    "        np.random.seed(42);\n",
    "        pl.seed_everything(42);\n",
    "        \n",
    "        folder_path2=f'Store{store}/Boruta-RF'\n",
    "        try:\n",
    "            os.mkdir(f'{folder_path1}/{folder_path2}')\n",
    "        except:\n",
    "            pass\n",
    "        \n",
    "        Rossmann_df = Rossmann[Rossmann['Store'] == store]\n",
    "        estimator = RandomForestRegressor(n_estimators=100,\n",
    "                                          min_samples_split=2,\n",
    "                                          min_samples_leaf=1,\n",
    "                                          max_depth=3,\n",
    "                                          criterion='squared_error')\n",
    "        boruta = BorutaPy(estimator = estimator,n_estimators = 30, max_iter = 100)\n",
    "        \n",
    "        \n",
    "        features  = list(Rossmann_df.columns)\n",
    "        train_df,train_sequences,test_sequences,train_size,train_dataset,test_dataset,scaler=pep_proc_summarizer(features,Rossmann_df)\n",
    "        data = dict_to_dataframe(train_dataset, SEQUENCE_LENGTH)\n",
    "\n",
    "\n",
    "        iteration_start = time.monotonic()\n",
    "        boruta.fit(np.array(data.iloc[:,:-1]),np.array(data.iloc[:,-1]))\n",
    "        iteration_end = time.monotonic()\n",
    "\n",
    "        important = list(data.iloc[:,:-1].columns[boruta.support_])\n",
    "        important = [i[0][11:] for i in important]\n",
    "        important = list(set(important)) \n",
    "\n",
    "        unimportant = list(data.iloc[:,:-1].columns[boruta.support_weak_])\n",
    "        unimportant = [i[0][11:] for i in unimportant]\n",
    "        unimportant = list(set(unimportant))\n",
    "\n",
    "        tentative = list(data.iloc[:,:-1].columns[~(boruta.support_|boruta.support_weak_)])\n",
    "        tentative = [i[0][11:] for i in tentative]\n",
    "        tentative = list(set(tentative)) \n",
    "\n",
    "        for i in important:\n",
    "            if i in unimportant:\n",
    "                unimportant.remove(i)\n",
    "        for i in important:\n",
    "            if i in tentative:\n",
    "                tentative.remove(i)\n",
    "        for i in unimportant:\n",
    "            if i in tentative:\n",
    "                tentative.remove(i)\n",
    "\n",
    "        important_df = pd.DataFrame(important, columns=[f'important'])\n",
    "        unimportant_df = pd.DataFrame(unimportant, columns=[f'unimportant'])\n",
    "        tentative_df = pd.DataFrame(tentative, columns=[f'tentative'])\n",
    "        \n",
    "        BorutaRF_df = pd.DataFrame(list(Rossmann_df.columns),columns = ['all_features'])\n",
    "        BorutaRF_df = BorutaRF_df.join([important_df,unimportant_df, tentative_df],how='left')\n",
    "        BorutaRF_df.to_csv(f\"{folder_path1}/{folder_path2}/BorutaRF_df.csv\",index = False)\n",
    "\n",
    "        run_time.append(iteration_end - iteration_start)\n",
    "        \n",
    "    durations['CompCost'] = run_time\n",
    "    duration_df =  pd.DataFrame.from_dict(durations)\n",
    "    duration_df.to_csv(f\"02-Results/Boruta-RF_time.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37304718",
   "metadata": {},
   "source": [
    "## BorutaSHAP-GB Feature Selection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c65f3e76",
   "metadata": {},
   "outputs": [],
   "source": [
    "if BorutaSHAPGB == True:\n",
    "    durations={}\n",
    "    run_time = []\n",
    "    for store in tqdm(Rosmann_stores):\n",
    "        torch.manual_seed(42);\n",
    "        np.random.seed(42);\n",
    "        pl.seed_everything(42);\n",
    "        \n",
    "        folder_path2=f'Store{store}/BorutaSHAP-GB'\n",
    "        try:\n",
    "            os.mkdir(f'{folder_path1}/{folder_path2}')\n",
    "        except:\n",
    "            pass\n",
    "        \n",
    "        Rossmann_df = Rossmann[Rossmann['Store'] == store]\n",
    "        estimator = GradientBoostingRegressor(n_estimators=100,\n",
    "                                              max_depth=3,\n",
    "                                              learning_rate=0.1,\n",
    "                                              loss='squared_error',\n",
    "                                              criterion='friedman_mse',\n",
    "                                              min_samples_split=2,\n",
    "                                              min_samples_leaf=1)\n",
    "    \n",
    "        selector = BorutaShap(estimator,importance_measure = 'shap', classification = False)     \n",
    "    \n",
    "    \n",
    "        features  = list(Rossmann_df.columns)\n",
    "        train_df,train_sequences,test_sequences,train_size,train_dataset,test_dataset,scaler=pep_proc_summarizer(features,Rossmann_df)\n",
    "        data = dict_to_dataframeBoSHAP(train_dataset, SEQUENCE_LENGTH)\n",
    "\n",
    "        iteration_start = time.monotonic()\n",
    "        with io.capture_output() as captured:\n",
    "            selector.fit(data.iloc[:,:-1], data.iloc[:,-1], n_trials=20,\n",
    "                        sample=False, train_or_test='train');\n",
    "        iteration_end = time.monotonic()\n",
    "        \n",
    "        important = list(selector.accepted)\n",
    "        important = [i[11:] for i in important]\n",
    "        important = list(set(important)) \n",
    "\n",
    "        unimportant = list(selector.rejected)\n",
    "        unimportant = [i[11:] for i in unimportant]\n",
    "        unimportant = list(set(unimportant))\n",
    "        \n",
    "        tentative = list(selector.tentative)\n",
    "        tentative = [i[11:] for i in tentative]\n",
    "        tentative = list(set(tentative))\n",
    "\n",
    "        \n",
    "        for i in important:\n",
    "            if i in unimportant:\n",
    "                unimportant.remove(i)\n",
    "        for i in important:\n",
    "            if i in tentative:\n",
    "                tentative.remove(i)\n",
    "        for i in unimportant:\n",
    "            if i in tentative:\n",
    "                tentative.remove(i)\n",
    "\n",
    "        important_df = pd.DataFrame(important, columns=[f'important'])\n",
    "        unimportant_df = pd.DataFrame(unimportant, columns=[f'unimportant'])\n",
    "        tentative_df = pd.DataFrame(tentative, columns=[f'tentative'])\n",
    "        \n",
    "        borutaSHAPGB_df = pd.DataFrame(list(Rossmann_df.columns),columns = ['all_features'])\n",
    "        borutaSHAPGB_df = borutaSHAPGB_df.join([important_df,unimportant_df, tentative_df],how='left')\n",
    "        borutaSHAPGB_df.to_csv(f\"{folder_path1}/{folder_path2}/borutaSHAPGB_df.csv\",index = False)\n",
    "\n",
    "        run_time.append(iteration_end - iteration_start)\n",
    "        \n",
    "    durations['CompCost'] = run_time\n",
    "    duration_df =  pd.DataFrame.from_dict(durations)\n",
    "    duration_df.to_csv(f\"02-Results/BorutaSHAP-GB_time.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "75c63e14",
   "metadata": {},
   "source": [
    "## BorutaSHAP-RF Feature Selection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ef95d52",
   "metadata": {},
   "outputs": [],
   "source": [
    "if BorutaSHAPRF == True:  \n",
    "    durations={}\n",
    "    run_time = []\n",
    "    for store in tqdm(Rosmann_stores):\n",
    "        torch.manual_seed(42);\n",
    "        np.random.seed(42);\n",
    "        pl.seed_everything(42);\n",
    "\n",
    "        folder_path2=f'Store{store}/BorutaSHAP-RF'\n",
    "        \n",
    "        try:\n",
    "            os.mkdir(f'{folder_path1}/{folder_path2}')\n",
    "        except:\n",
    "            pass\n",
    "        \n",
    "        Rossmann_df = Rossmann[Rossmann['Store'] == store]\n",
    "        estimator = RandomForestRegressor(n_estimators=100,\n",
    "                                          min_samples_split=2,\n",
    "                                          min_samples_leaf=1,\n",
    "                                          max_depth=3,\n",
    "                                          criterion='squared_error')\n",
    "    \n",
    "        selector = BorutaShap(estimator,importance_measure = 'shap', classification = False)     \n",
    "    \n",
    "    \n",
    "        features  = list(Rossmann_df.columns)\n",
    "        train_df,train_sequences,test_sequences,train_size,train_dataset,test_dataset,scaler=pep_proc_summarizer(features,Rossmann_df)\n",
    "        data = dict_to_dataframeBoSHAP(train_dataset, SEQUENCE_LENGTH)\n",
    "\n",
    "        iteration_start = time.monotonic()\n",
    "        with io.capture_output() as captured:\n",
    "            selector.fit(data.iloc[:,:-1], data.iloc[:,-1], n_trials=20,\n",
    "                        sample=False, train_or_test='train');\n",
    "        iteration_end = time.monotonic()\n",
    "        \n",
    "        important = list(selector.accepted)\n",
    "        important = [i[11:] for i in important]\n",
    "        important = list(set(important)) \n",
    "\n",
    "        unimportant = list(selector.rejected)\n",
    "        unimportant = [i[11:] for i in unimportant]\n",
    "        unimportant = list(set(unimportant))\n",
    "        \n",
    "        tentative = list(selector.tentative)\n",
    "        tentative = [i[11:] for i in tentative]\n",
    "        tentative = list(set(tentative))\n",
    "\n",
    "        \n",
    "        for i in important:\n",
    "            if i in unimportant:\n",
    "                unimportant.remove(i)\n",
    "        for i in important:\n",
    "            if i in tentative:\n",
    "                tentative.remove(i)\n",
    "        for i in unimportant:\n",
    "            if i in tentative:\n",
    "                tentative.remove(i)\n",
    "\n",
    "        important_df = pd.DataFrame(important, columns=[f'important'])\n",
    "        unimportant_df = pd.DataFrame(unimportant, columns=[f'unimportant'])\n",
    "        tentative_df = pd.DataFrame(tentative, columns=[f'tentative'])\n",
    "        \n",
    "        borutaSHAPRF_df = pd.DataFrame(list(Rossmann_df.columns),columns = ['all_features'])\n",
    "        borutaSHAPRF_df = borutaSHAPRF_df.join([important_df,unimportant_df, tentative_df],how='left')\n",
    "        borutaSHAPRF_df.to_csv(f\"{folder_path1}/{folder_path2}/borutaSHAPRF_df.csv\",index = False)\n",
    "\n",
    "        run_time.append(iteration_end - iteration_start)\n",
    "        \n",
    "    durations['CompCost'] = run_time\n",
    "    duration_df =  pd.DataFrame.from_dict(durations)\n",
    "    duration_df.to_csv(f\"02-Results/BorutaSHAP-RF_time.csv\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1030284d",
   "metadata": {},
   "source": [
    "## SHAP-LSTM FS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9144bb47",
   "metadata": {},
   "outputs": [],
   "source": [
    "if LSTM_SHAP == True: \n",
    "    durations={}\n",
    "    run_time = []\n",
    "    for store in tqdm(Rosmann_stores):\n",
    "        torch.manual_seed(42);\n",
    "        np.random.seed(42);\n",
    "        pl.seed_everything(42);  \n",
    "        \n",
    "        \n",
    "        Rossmann_df = Rossmann[Rossmann['Store'] == store]\n",
    "        features  = list(Rossmann_df.columns)\n",
    "        train_df,train_sequences,test_sequences,train_size,train_dataset,test_dataset,scaler=pep_proc_summarizer(features,Rossmann_df)\n",
    "        \n",
    "        folder_path2=f'Store{store}/SHAP-LSTM'\n",
    "        \n",
    "        try:\n",
    "            os.mkdir(f'{folder_path1}/{folder_path2}')\n",
    "        except:\n",
    "            pass\n",
    "        \n",
    "        kmeans = shap.kmeans(train_df,5)\n",
    "        #explainer\n",
    "        \n",
    "        trained_model = Predictor.load_from_checkpoint(\n",
    "        f'02-Results/LSTM/Store{store}/All_Features/Checkpoints/Rossmann_All_LSTM_epoch=199.ckpt',\n",
    "        n_features = train_df.shape[1]\n",
    "        )\n",
    "\n",
    "        iteration_start = time.monotonic()\n",
    "        explainer = shap.KernelExplainer(LSTM_predict, kmeans)\n",
    "        #explanation\n",
    "        shap_values = explainer.shap_values(train_df[-365:])\n",
    "        iteration_end = time.monotonic()\n",
    "        \n",
    "        \n",
    "        #return a FEATURES_TO_USE numpber of features ordered by average shap value\n",
    "        sorted_avg_shap, top_features_dic = return_shap_features(shap_values,train_df,30)\n",
    "        shap_per_feature = pd.DataFrame.from_dict(sorted_avg_shap, orient='index', columns=['Importance'])\n",
    "        shap_per_feature.to_csv(f\"{folder_path1}/{folder_path2}/avg_SHAP_LSTM.csv\")\n",
    "        max_shap_df = return_shap_max (shap_values,train_df)\n",
    "        max_shap_df.to_csv(f\"{folder_path1}/{folder_path2}/inst_SHAP_LSTM.csv\")\n",
    "        ExplanationSHAP = shap.Explanation(values=shap_values, data=train_df[-365:])\n",
    "\n",
    "\n",
    "        run_time.append(iteration_end - iteration_start)\n",
    "        \n",
    "    durations['CompCost'] = run_time\n",
    "    duration_df =  pd.DataFrame.from_dict(durations)\n",
    "    duration_df.to_csv(f\"02-Results/SHAP_LSTM_time.csv\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a6b22844",
   "metadata": {},
   "source": [
    "## LIME-LSTM FS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca21445d",
   "metadata": {},
   "outputs": [],
   "source": [
    "if LSTM_LIME_ == True:\n",
    "    durations={}\n",
    "    run_time = []\n",
    "    for store in tqdm(Rosmann_stores):\n",
    "        torch.manual_seed(42);\n",
    "        np.random.seed(42);\n",
    "        pl.seed_everything(42); \n",
    "        \n",
    "        Rossmann_df = Rossmann[Rossmann['Store'] == store]\n",
    "        features  = list(Rossmann_df.columns)\n",
    "        train_df,train_sequences,test_sequences,train_size,train_dataset,test_dataset,scaler=pep_proc_summarizer(features,Rossmann_df)\n",
    "                \n",
    "        dic={}\n",
    "        trained_model = Predictor.load_from_checkpoint(\n",
    "        f'02-Results/LSTM/Store{store}/All_Features/Checkpoints/Rossmann_All_LSTM_epoch=199.ckpt',\n",
    "        n_features = train_df.shape[1]\n",
    "        )\n",
    "        \n",
    "        \n",
    "        folder_path2=f'Store{store}/LIME-LSTM'\n",
    "        try:\n",
    "            os.mkdir(f'{folder_path1}/{folder_path2}')\n",
    "        except:\n",
    "            pass\n",
    "        \n",
    "        iteration_start = time.monotonic()\n",
    "        explainer = shap.explainers.other.LimeTabular(LSTM_predict,train_df[-365:],'regression')\n",
    "        lime = LIME_Explainer(60)\n",
    "        iteration_end = time.monotonic()\n",
    "        \n",
    "        lime_df = pd.DataFrame(lime)\n",
    "        lime_df_single = np.abs(lime_df)\n",
    "\n",
    "        for col in lime_df.columns:\n",
    "            lime_df_single[col] = np.sum(np.abs(lime_df_single[col]))/len(lime_df_single[col])\n",
    "            \n",
    "        lime_df_single = lime_df_single.iloc[0]\n",
    "        lime_df_single  = lime_df_single.sort_values(ascending=False)\n",
    "\n",
    "        lime_df_single.to_csv(f\"{folder_path1}/{folder_path2}/LIME-LSTM.csv\", header = ['LIME_value'],index_label=\"Features\")\n",
    "        \n",
    "        for col in lime_df.columns:\n",
    "            dic[col]=[]\n",
    "        for i in range(2,len(lime_df)):\n",
    "            for col in lime_df.columns:\n",
    "                dic[col].append(np.sum(np.abs(lime_df[col].iloc[:i]))/len(lime_df[col].iloc[:i]))\n",
    "                \n",
    "        large_n_rule =  pd.DataFrame.from_dict(dic)\n",
    "        large_n_rule.to_csv(f\"{folder_path1}/{folder_path2}/large_n_rule.csv\",index = False)\n",
    "        run_time.append(iteration_end - iteration_start)\n",
    "\n",
    "    durations['CompCost'] = run_time\n",
    "    duration_df =  pd.DataFrame.from_dict(durations)\n",
    "    duration_df.to_csv(f\"02-Results/LIME_LSTM_time.csv\")\n",
    "    \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb2a830c",
   "metadata": {},
   "source": [
    "## IMV-LSTM "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee149417",
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprop_imv(store):\n",
    "    df_og = Rossmann[Rossmann['Store'] == store]\n",
    "\n",
    "    data1=df_og.iloc[:-89,:]\n",
    "    data2=df_og.iloc[-89:,:]\n",
    "\n",
    "    target = TGT\n",
    "    cols = list(data1.columns)\n",
    "\n",
    "    train_size = data1.shape[0]\n",
    "    val_size = 89\n",
    "    depth = 15\n",
    "    batch_size = 7\n",
    "    prediction_horizon = 1\n",
    "\n",
    "    X_train1 = np.zeros((len(data1), depth, len(cols)))\n",
    "    y_train1 = np.zeros((len(data1), 1))\n",
    "\n",
    "    for i, name in enumerate(cols):\n",
    "        for j in range(depth):\n",
    "            X_train1[:, j, i] = data1[name].shift(depth - j - 1).fillna(method=\"bfill\")\n",
    "    y_train1 = data1[target].shift(-prediction_horizon).fillna(method='ffill')\n",
    "\n",
    "    X_train1 = X_train1[depth:-prediction_horizon]\n",
    "    y_train1 = y_train1[depth:-prediction_horizon]\n",
    "\n",
    "    X2 = np.zeros((len(data2), depth, len(cols)))\n",
    "    y2 = np.zeros((len(data2), 1))\n",
    "\n",
    "    for i, name in enumerate(cols):\n",
    "        for j in range(depth):\n",
    "            X2[:, j, i] = data2[name].shift(depth - j - 1).fillna(method=\"bfill\")\n",
    "    y2 = data2[target].shift(-prediction_horizon).fillna(method='ffill')\n",
    "\n",
    "    X_train2 = X2[:train_size - len(data1)]\n",
    "    y_train2 = y2[:train_size - len(data1)]\n",
    "\n",
    "    X_val = X2[train_size - len(data1):train_size - len(data1) + val_size]\n",
    "    y_val = y2[train_size - len(data1):train_size - len(data1) + val_size]\n",
    "\n",
    "    X_test = X2[train_size - len(data1) + val_size:]\n",
    "    y_test = y2[train_size - len(data1) + val_size:]\n",
    "\n",
    "    X_train2 = X_train2[depth:]\n",
    "    y_train2 = y_train2[depth:]\n",
    "\n",
    "    X_train = np.concatenate([X_train1, X_train2], axis=0)\n",
    "    y_train = np.concatenate([y_train1, y_train2], axis=0)\n",
    "\n",
    "    X_train_min, y_train_min = X_train.min(axis=0), y_train.min(axis=0)\n",
    "    X_train_max, y_train_max = X_train.max(axis=0), y_train.max(axis=0)\n",
    "\n",
    "    X_train = (X_train - X_train_min)/(X_train_max - X_train_min + 1e-9)\n",
    "    X_val = (X_val - X_train_min)/(X_train_max - X_train_min + 1e-9)\n",
    "    X_test = (X_test - X_train_min)/(X_train_max - X_train_min + 1e-9)\n",
    "\n",
    "    y_train = (y_train - y_train_min)/(y_train_max - y_train_min + 1e-9)\n",
    "    y_val = (y_val - y_train_min)/(y_train_max - y_train_min + 1e-9)\n",
    "    y_test = (y_test - y_train_min)/(y_train_max - y_train_min + 1e-9)\n",
    "\n",
    "    X_train_t = torch.Tensor(X_train)\n",
    "    X_val_t = torch.Tensor(X_val)\n",
    "    X_test_t = torch.Tensor(X_test)\n",
    "    y_train_t = torch.Tensor(y_train)\n",
    "    y_val_t = torch.Tensor(y_val.values)\n",
    "    y_test_t = torch.Tensor(y_test.values)\n",
    "\n",
    "    train_loader = DataLoader(TensorDataset(X_train_t, y_train_t), shuffle=True, batch_size=batch_size)\n",
    "    val_loader = DataLoader(TensorDataset(X_val_t, y_val_t), shuffle=False, batch_size=batch_size)\n",
    "    test_loader = DataLoader(TensorDataset(X_test_t, y_test_t), shuffle=False, batch_size=batch_size)\n",
    "    \n",
    "    return(X_train,train_loader,val_loader,test_loader,X_train_t,X_val_t,X_test_t,y_train_t,y_val_t,y_test_t,y_train_max,y_train_min,cols)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4c8d140",
   "metadata": {},
   "outputs": [],
   "source": [
    "epochs = 100\n",
    "loss = nn.MSELoss()\n",
    "patience = 20\n",
    "\n",
    "\n",
    "if IMV_Tensor == True:\n",
    "    durations={}\n",
    "    run_time = []\n",
    "    for store in tqdm(Rosmann_stores):\n",
    "        torch.manual_seed(42);\n",
    "        np.random.seed(42);\n",
    "        pl.seed_everything(42); \n",
    "        min_val_loss = 9999\n",
    "        counter = 0\n",
    "        X_train,train_loader,val_loader,test_loader,X_train_t,X_val_t,X_test_t,y_train_t,y_val_t,y_test_t,y_train_max,y_train_min,cols = preprop_imv(store)\n",
    "\n",
    "\n",
    "        folder_path2=f'Store{store}/IMV-LSTM'\n",
    "        try:\n",
    "            os.mkdir(f'{folder_path1}/{folder_path2}')\n",
    "        except:\n",
    "            pass\n",
    "\n",
    "        model = IMVTensorLSTM(X_train.shape[2], 1, 32)\n",
    "        opt = torch.optim.Adam(model.parameters(), lr=0.05)\n",
    "        epoch_scheduler = torch.optim.lr_scheduler.StepLR(opt, 20, gamma=0.9)\n",
    "\n",
    "        iteration_start = time.monotonic()\n",
    "        for i in range(epochs):\n",
    "            mse_train = 0\n",
    "            for batch_x, batch_y in train_loader :\n",
    "                batch_x = batch_x\n",
    "                batch_y = batch_y\n",
    "                opt.zero_grad()\n",
    "                y_pred, alphas_Tensor, betas_Tensor = model(batch_x)\n",
    "                y_pred = y_pred.squeeze(1)\n",
    "                l = loss(y_pred, batch_y)\n",
    "                l.backward()\n",
    "                mse_train += l.item()*batch_x.shape[0]\n",
    "                opt.step()\n",
    "            epoch_scheduler.step()\n",
    "            #with torch.no_grad():\n",
    "            mse_val = 0\n",
    "            preds = []\n",
    "            true = []\n",
    "            alphas_Tensor = []\n",
    "            betas_Tensor = []\n",
    "\n",
    "            for batch_x, batch_y in val_loader:\n",
    "                batch_x = batch_x\n",
    "                batch_y = batch_y\n",
    "                output, a, b = model(batch_x)\n",
    "                output = output.squeeze(1)\n",
    "                preds.append(output.detach().cpu().numpy())\n",
    "                true.append(batch_y.detach().cpu().numpy())\n",
    "                alphas_Tensor.append(a.detach().cpu().numpy())\n",
    "                betas_Tensor.append(b.detach().cpu().numpy())\n",
    "                mse_val += loss(output, batch_y).item()*batch_x.shape[0]\n",
    "            preds = np.concatenate(preds)\n",
    "            true = np.concatenate(true)\n",
    "            alphas_Tensor = np.concatenate(alphas_Tensor)\n",
    "            betas_Tensor = np.concatenate(betas_Tensor)\n",
    "\n",
    "            if min_val_loss > mse_val**0.5:\n",
    "                min_val_loss = mse_val**0.5\n",
    "                #print(\"Saving...\")\n",
    "                torch.save(model.state_dict(), \"imv_full_lstm.pt\")\n",
    "                counter = 0\n",
    "            else: \n",
    "                counter += 1\n",
    "\n",
    "            if counter == patience:\n",
    "                preds = preds*(y_train_max - y_train_min) + y_train_min\n",
    "                true = true*(y_train_max - y_train_min) + y_train_min\n",
    "                mse = mean_squared_error(true, preds)\n",
    "                mae = mean_absolute_error(true, preds)\n",
    "                print(\"mse: \", mse, \"mae: \", mae)\n",
    "                break\n",
    "    \n",
    "            if(i % 10 == 0):\n",
    "                preds = preds*(y_train_max - y_train_min) + y_train_min\n",
    "                true = true*(y_train_max - y_train_min) + y_train_min\n",
    "                mse = mean_squared_error(true, preds)\n",
    "                mae = mean_absolute_error(true, preds)\n",
    "                #print(\"mse: \", mse, \"mae: \", mae)\n",
    "\n",
    "        iteration_end = time.monotonic()\n",
    "\n",
    "        alphas_Tensor = alphas_Tensor.mean(axis=0)\n",
    "        betas_Tensor = betas_Tensor.mean(axis=0)\n",
    "\n",
    "        alphas_Tensor = alphas_Tensor[..., 0]\n",
    "        betas_Tensor = betas_Tensor[..., 0]\n",
    "\n",
    "        alphas_Tensor = alphas_Tensor.transpose(1, 0)\n",
    "\n",
    "        X_train_t = torch.Tensor(X_train)\n",
    "\n",
    "        IMV_T_dic = {'features': cols , 'Importance': betas_Tensor} \n",
    "        IMV_T_df = pd.DataFrame(IMV_T_dic)\n",
    "        IMV_T_df  = IMV_T_df.sort_values(ascending=False, by ='Importance')\n",
    "\n",
    "        IMV_T_df.to_csv(f\"{folder_path1}/{folder_path2}/IMV_Tensor.csv\")\n",
    "\n",
    "        run_time.append(iteration_end - iteration_start)\n",
    "\n",
    "    durations['CompCost'] = run_time\n",
    "    duration_df =  pd.DataFrame.from_dict(durations)\n",
    "    duration_df.to_csv(f\"02-Results/IMV_Tensor_time.csv\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dde0f8d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "epochs = 100\n",
    "loss = nn.MSELoss()\n",
    "patience = 20\n",
    "\n",
    "\n",
    "if IMV_Full == True:\n",
    "    durations={}\n",
    "    run_time = []\n",
    "    for store in tqdm(Rosmann_stores):\n",
    "        torch.manual_seed(42);\n",
    "        np.random.seed(42);\n",
    "        pl.seed_everything(42); \n",
    "        min_val_loss = 9999\n",
    "        counter = 0\n",
    "        X_train,train_loader,val_loader,test_loader,X_train_t,X_val_t,X_test_t,y_train_t,y_val_t,y_test_t,y_train_max,y_train_min,cols = preprop_imv(store)\n",
    "\n",
    "\n",
    "        folder_path2=f'Store{store}/IMV-LSTM'\n",
    "        try:\n",
    "            os.mkdir(f'{folder_path1}/{folder_path2}')\n",
    "        except:\n",
    "            pass\n",
    "\n",
    "        model = IMVFullLSTM(X_train.shape[2], 1, 32)\n",
    "        opt = torch.optim.Adam(model.parameters(), lr=0.05)\n",
    "        epoch_scheduler = torch.optim.lr_scheduler.StepLR(opt, 20, gamma=0.9)\n",
    "\n",
    "        iteration_start = time.monotonic()\n",
    "        for i in (range(epochs)):\n",
    "            mse_train = 0\n",
    "            for batch_x, batch_y in train_loader :\n",
    "                batch_x = batch_x\n",
    "                batch_y = batch_y\n",
    "                opt.zero_grad()\n",
    "                y_pred, alphas_Tensor, betas_Tensor = model(batch_x)\n",
    "                y_pred = y_pred.squeeze(1)\n",
    "                l = loss(y_pred, batch_y)\n",
    "                l.backward()\n",
    "                mse_train += l.item()*batch_x.shape[0]\n",
    "                opt.step()\n",
    "            epoch_scheduler.step()\n",
    "            #with torch.no_grad():\n",
    "            mse_val = 0\n",
    "            preds = []\n",
    "            true = []\n",
    "            alphas_Tensor = []\n",
    "            betas_Tensor = []\n",
    "\n",
    "            for batch_x, batch_y in val_loader:\n",
    "                batch_x = batch_x\n",
    "                batch_y = batch_y\n",
    "                output, a, b = model(batch_x)\n",
    "                output = output.squeeze(1)\n",
    "                preds.append(output.detach().cpu().numpy())\n",
    "                true.append(batch_y.detach().cpu().numpy())\n",
    "                alphas_Tensor.append(a.detach().cpu().numpy())\n",
    "                betas_Tensor.append(b.detach().cpu().numpy())\n",
    "                mse_val += loss(output, batch_y).item()*batch_x.shape[0]\n",
    "            preds = np.concatenate(preds)\n",
    "            true = np.concatenate(true)\n",
    "            alphas_Tensor = np.concatenate(alphas_Tensor)\n",
    "            betas_Tensor = np.concatenate(betas_Tensor)\n",
    "\n",
    "            if min_val_loss > mse_val**0.5:\n",
    "                min_val_loss = mse_val**0.5\n",
    "                #print(\"Saving...\")\n",
    "                torch.save(model.state_dict(), \"imv_full_lstm.pt\")\n",
    "                counter = 0\n",
    "            else: \n",
    "                counter += 1\n",
    "\n",
    "            if counter == patience:\n",
    "                preds = preds*(y_train_max - y_train_min) + y_train_min\n",
    "                true = true*(y_train_max - y_train_min) + y_train_min\n",
    "                mse = mean_squared_error(true, preds)\n",
    "                mae = mean_absolute_error(true, preds)\n",
    "                print(\"mse: \", mse, \"mae: \", mae)\n",
    "                break\n",
    "            #print(\"Iter: \", i, \"train: \", (mse_train/len(X_train_t))**0.5, \"val: \", (mse_val/len(X_val_t))**0.5)\n",
    "            if(i % 10 == 0):\n",
    "                preds = preds*(y_train_max - y_train_min) + y_train_min\n",
    "                true = true*(y_train_max - y_train_min) + y_train_min\n",
    "                mse = mean_squared_error(true, preds)\n",
    "                mae = mean_absolute_error(true, preds)\n",
    "                #print(\"mse: \", mse, \"mae: \", mae)\n",
    "\n",
    "        iteration_end = time.monotonic()\n",
    "\n",
    "        alphas_Tensor = alphas_Tensor.mean(axis=0)\n",
    "        betas_Tensor = betas_Tensor.mean(axis=0)\n",
    "\n",
    "        alphas_Tensor = alphas_Tensor[..., 0]\n",
    "        betas_Tensor = betas_Tensor[..., 0]\n",
    "\n",
    "        alphas_Tensor = alphas_Tensor.transpose(1, 0)\n",
    "\n",
    "        X_train_t = torch.Tensor(X_train)\n",
    "\n",
    "        IMV_T_dic = {'features': cols , 'Importance': betas_Tensor} \n",
    "        IMV_T_df = pd.DataFrame(IMV_T_dic)\n",
    "        IMV_T_df  = IMV_T_df.sort_values(ascending=False, by ='Importance')\n",
    "\n",
    "        IMV_T_df.to_csv(f\"{folder_path1}/{folder_path2}/IMV_Full.csv\")\n",
    "\n",
    "        run_time.append(iteration_end - iteration_start)\n",
    "\n",
    "    durations['CompCost'] = run_time\n",
    "    duration_df =  pd.DataFrame.from_dict(durations)\n",
    "    duration_df.to_csv(f\"02-Results/IMV_Full_time.csv\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53515d63",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
